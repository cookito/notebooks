{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch #基本モジュール\n",
    "from torch.autograd import Variable #自動微分用\n",
    "import torch.nn as nn #ネットワーク構築用\n",
    "import torch.optim as optim #最適化関数\n",
    "import torch.nn.functional as F #ネットワーク用の様々な関数\n",
    "import torch.utils.data #データセット読み込み関連\n",
    "import torchvision #画像関連\n",
    "from torchvision import datasets, models, transforms #画像用データセット諸々\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "import logzero\n",
    "from logzero import logger\n",
    "\n",
    "\n",
    "# examination settings\n",
    "batch_size = 128\n",
    "test_batch_size = batch_size * 2\n",
    "epochs = 1000\n",
    "lr = 0.5\n",
    "momentum = 0.9\n",
    "seed = 2017\n",
    "log_interval = 1000\n",
    "num_workers = 2\n",
    "use_gpu = False\n",
    "ex_name = 'pseudo'\n",
    "\n",
    "# set seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# logger setting\n",
    "LOG_FORMAT = '[%(asctime)s %(levelname)s] %(message)s'\n",
    "logzero.loglevel(logging.INFO)\n",
    "logzero.logfile('./log/{}.log'.format(ex_name))\n",
    "logzero.formatter(logging.Formatter(LOG_FORMAT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('./data/MNIST/train/', download=True, train=True)\n",
    "test_dataset = datasets.MNIST('./data/MNIST/test/', download=True, train=False)\n",
    "\n",
    "train_data = train_dataset.train_data.numpy()\n",
    "test_data = test_dataset.test_data.numpy()\n",
    "train_labels = train_dataset.train_labels.numpy()\n",
    "test_labels = test_dataset.test_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels), len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the ratio train/test\n",
    "train:val:test を 800:200:69000に設定。各クラスのサンプル数は均一に。testセットは実際に教師なし学習に用いることは想定しづらいので、現実のタスクや論文に書く時はunlabeledデータとして説明すれば良い。その場合はtestセット内で更に unlabeled/holdout に分ける必要がある。\n",
    "交差検証は行わない。\n",
    "\n",
    "pseudo label実験に対してはtestデータはunlabeledデータ及び最終評価用のデータとして使われる。valは学習中の精度監視用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28, 1) (1000, 10) (1000, 28, 28, 1) (1000, 10) (68000, 28, 28, 1) (68000, 10)\n"
     ]
    }
   ],
   "source": [
    "n_train = 1000\n",
    "n_val = 1000\n",
    "n_test = 68000\n",
    "\n",
    "X_train = np.empty(shape=(0, 28, 28))\n",
    "y_train = np.empty(shape=(0,))\n",
    "X_val = np.empty(shape=(0, 28, 28))\n",
    "y_val = np.empty(shape=(0,))\n",
    "X_test = np.empty(shape=(0, 28, 28))\n",
    "y_test = np.empty(shape=(0,))\n",
    "\n",
    "for i in range(10):\n",
    "    idx_train = np.argwhere(train_labels == i)[:(n_train//10)].squeeze()\n",
    "    idx_val = np.argwhere(train_labels == i)[(n_train//10):((n_train+n_val)//10)].squeeze()\n",
    "    idx_test = np.argwhere(train_labels == i)[((n_train+n_val)//10):].squeeze()\n",
    "    \n",
    "    X_train = np.concatenate([X_train, train_data[idx_train]], axis=0)\n",
    "    y_train = np.concatenate([y_train, train_labels[idx_train]], axis=0)\n",
    "    X_val = np.concatenate([X_val, train_data[idx_val]], axis=0)\n",
    "    y_val = np.concatenate([y_val, train_labels[idx_val]], axis=0)\n",
    "    X_test = np.concatenate([X_test, train_data[idx_test]], axis=0)\n",
    "    y_test = np.concatenate([y_test, train_labels[idx_test]], axis=0)\n",
    "    \n",
    "X_test = np.concatenate([X_test, test_data], axis=0)\n",
    "y_test = np.concatenate([y_test, test_labels], axis=0)\n",
    "\n",
    "# treat as image\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_val = np.expand_dims(X_val, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "# one-hot encoding\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_val = lb.transform(y_val)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Pseudo Label\n",
    "\n",
    "testデータをtest/unlabeledに分ける。\n",
    "比率は10000:58000とする。（元々のMNISTの提供時点でのtest splitを用いる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1) (10000, 10) (58000, 10) (0, 28, 28, 1) (0, 10)\n"
     ]
    }
   ],
   "source": [
    "y_pseudo = np.load('./save/pseudo_label.npy')\n",
    "\n",
    "X_unlabeled = X_test[:58000]\n",
    "y_unlabeled = y_test[:58000]\n",
    "y_pseudo = y_pseudo[:58000]\n",
    "X_test = X_test[58000:]\n",
    "y_test = y_test[58000:]\n",
    "\n",
    "print(X_unlabeled.shape, y_unlabeled.shape, y_pseudo.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.fc = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.bn2(self.conv2(x)), 2))\n",
    "        x = F.adaptive_max_pool2d(x, output_size=1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.X[index]\n",
    "        img = self.transform(img)\n",
    "        target = torch.Tensor(self.y[index].astype(float))\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "\n",
    "class MNISTDatasetForPseudoLabeling(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_train, y_train, X_dash, Y_dash, transform):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.X[index]\n",
    "        img = self.transform(img)\n",
    "        target = torch.Tensor(self.y[index].astype(float))\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    \n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = MNISTDataset(X_train, y_train, transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_dataset = MNISTDataset(X_val, y_val, transform)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=test_batch_size, num_workers=num_workers)\n",
    "test_dataset = MNISTDataset(X_test, y_test, transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define TrainingTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingTracker(object):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.history = {\n",
    "            'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []\n",
    "        }\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.train_acc = 0\n",
    "        self.train_loss = 0\n",
    "        self.val_acc = 0\n",
    "        self.val_loss = 0\n",
    "        self.train_count = 0\n",
    "        self.val_count = 0\n",
    "\n",
    "    def update(self, train, acc, loss, n):\n",
    "        if train:\n",
    "            acc_sum = self.train_acc * self.train_count + acc * n\n",
    "            loss_sum = self.train_loss * self.train_count + loss * n\n",
    "            self.train_count += n\n",
    "            self.train_acc = acc_sum / self.train_count\n",
    "            self.train_loss = loss_sum / self.train_count\n",
    "        else:\n",
    "            acc_sum = self.val_acc * self.val_count + acc * n\n",
    "            loss_sum = self.val_loss * self.val_count + loss * n\n",
    "            self.val_count += n\n",
    "            self.val_acc = acc_sum / self.val_count\n",
    "            self.val_loss = loss_sum / self.val_count\n",
    "\n",
    "    def plot_history(self):\n",
    "        plt.plot(self.history['train_acc'])\n",
    "        plt.plot(self.history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend(['train_acc', 'val_acc'], loc='lower right')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(self.history['train_loss'])\n",
    "        plt.plot(self.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend(['train_loss', 'val_loss'], loc='lower right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train, validate, test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _train(model, data_loader, criterion, optimizer, tracker):\n",
    "    model.train()\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        if use_gpu:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "        X, y = Variable(X), Variable(y)\n",
    "        \n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        pred = output.data.max(1)[1]\n",
    "        answer = y.data.max(1)[1]\n",
    "        acc = pred.eq(answer).sum() / len(X)\n",
    "        tracker.update(train=True, acc=acc, loss=loss.data[0], n=len(X))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    logger.info(\n",
    "        'Epoch: {tracker.epoch} | '\n",
    "        'TrainAcc: {tracker.train_acc:.4f} | '\n",
    "        'TrainLoss: {tracker.train_loss:.4f}'.format(\n",
    "            tracker=tracker)\n",
    "    )\n",
    "    tracker.history['train_acc'].append(tracker.train_acc)\n",
    "    tracker.history['train_loss'] .append(tracker.train_loss)\n",
    "    tracker.reset()\n",
    "\n",
    "def _validate(model, data_loader, criterion, tracker):\n",
    "    model.eval()\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        if use_gpu:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "        X, y = Variable(X), Variable(y)\n",
    "        \n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        pred = output.data.max(1)[1]\n",
    "        answer = y.data.max(1)[1]\n",
    "        acc = pred.eq(answer).sum() / len(X)\n",
    "        tracker.update(train=False, acc=acc, loss=loss.data[0], n=len(X))\n",
    "\n",
    "    logger.info(\n",
    "        'Epoch: {tracker.epoch} | '\n",
    "        'ValAcc: {tracker.val_acc:.4f} | '\n",
    "        'ValLoss: {tracker.val_loss:.4f} \\n'.format(\n",
    "            tracker=tracker)\n",
    "    )\n",
    "    tracker.history['val_acc'].append(tracker.val_acc)\n",
    "    tracker.history['val_loss'].append(tracker.val_loss)\n",
    "    tracker.reset()\n",
    "    \n",
    "def _test(model, data_loader, criterion, make_pl=False):\n",
    "    model.eval()\n",
    "    \n",
    "    if make_pl:\n",
    "        pseudo_label = np.empty((0, 10))\n",
    "    \n",
    "    acc_sum = 0\n",
    "    loss_sum = 0\n",
    "    for i, (X, y) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        if use_gpu:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "        X, y = Variable(X), Variable(y)\n",
    "        \n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        pred = output.data.max(1)[1]\n",
    "        answer = y.data.max(1)[1]\n",
    "        acc_sum += pred.eq(answer).sum()\n",
    "        loss_sum += loss.data[0] * X.size(0)\n",
    "        \n",
    "        if make_pl:\n",
    "            pseudo_label = np.concatenate([pseudo_label, output.data.numpy()], axis=0)\n",
    "\n",
    "    acc = acc_sum / n_test\n",
    "    loss = loss_sum / n_test\n",
    "    logger.info(\n",
    "        'TestAcc: {0:.4f} | '\n",
    "        'TestLoss: {1:.4f} '.format(\n",
    "            acc, loss)\n",
    "    )\n",
    "    \n",
    "    if make_pl:\n",
    "        np.save('./save/pseudo_label_{}.npy'.format(ex_name), pseudo_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:02:35,389 INFO] Epoch: 0 | TrainAcc: 0.1380 | TrainLoss: 0.2344\n",
      "[2017-12-02 18:02:35,826 INFO] Epoch: 0 | ValAcc: 0.2230 | ValLoss: 0.2284 \n",
      "\n",
      "[2017-12-02 18:02:36,624 INFO] Epoch: 1 | TrainAcc: 0.2480 | TrainLoss: 0.2096\n",
      "[2017-12-02 18:02:36,999 INFO] Epoch: 1 | ValAcc: 0.3580 | ValLoss: 0.2156 \n",
      "\n",
      "[2017-12-02 18:02:37,756 INFO] Epoch: 2 | TrainAcc: 0.4380 | TrainLoss: 0.1796\n",
      "[2017-12-02 18:02:38,115 INFO] Epoch: 2 | ValAcc: 0.5270 | ValLoss: 0.1742 \n",
      "\n",
      "[2017-12-02 18:02:38,852 INFO] Epoch: 3 | TrainAcc: 0.5530 | TrainLoss: 0.1407\n",
      "[2017-12-02 18:02:39,205 INFO] Epoch: 3 | ValAcc: 0.4420 | ValLoss: 0.1626 \n",
      "\n",
      "[2017-12-02 18:02:39,974 INFO] Epoch: 4 | TrainAcc: 0.6800 | TrainLoss: 0.1039\n",
      "[2017-12-02 18:02:40,351 INFO] Epoch: 4 | ValAcc: 0.4330 | ValLoss: 0.1578 \n",
      "\n",
      "[2017-12-02 18:02:41,092 INFO] Epoch: 5 | TrainAcc: 0.7120 | TrainLoss: 0.0870\n",
      "[2017-12-02 18:02:41,535 INFO] Epoch: 5 | ValAcc: 0.6110 | ValLoss: 0.1095 \n",
      "\n",
      "[2017-12-02 18:02:42,324 INFO] Epoch: 6 | TrainAcc: 0.7830 | TrainLoss: 0.0673\n",
      "[2017-12-02 18:02:42,689 INFO] Epoch: 6 | ValAcc: 0.7370 | ValLoss: 0.0813 \n",
      "\n",
      "[2017-12-02 18:02:43,441 INFO] Epoch: 7 | TrainAcc: 0.8190 | TrainLoss: 0.0559\n",
      "[2017-12-02 18:02:43,805 INFO] Epoch: 7 | ValAcc: 0.6500 | ValLoss: 0.1065 \n",
      "\n",
      "[2017-12-02 18:02:44,514 INFO] Epoch: 8 | TrainAcc: 0.8660 | TrainLoss: 0.0421\n",
      "[2017-12-02 18:02:44,862 INFO] Epoch: 8 | ValAcc: 0.7790 | ValLoss: 0.0684 \n",
      "\n",
      "[2017-12-02 18:02:45,563 INFO] Epoch: 9 | TrainAcc: 0.8930 | TrainLoss: 0.0357\n",
      "[2017-12-02 18:02:45,915 INFO] Epoch: 9 | ValAcc: 0.7970 | ValLoss: 0.0655 \n",
      "\n",
      "[2017-12-02 18:02:46,658 INFO] Epoch: 10 | TrainAcc: 0.9070 | TrainLoss: 0.0299\n",
      "[2017-12-02 18:02:47,084 INFO] Epoch: 10 | ValAcc: 0.7790 | ValLoss: 0.0657 \n",
      "\n",
      "[2017-12-02 18:02:47,922 INFO] Epoch: 11 | TrainAcc: 0.9090 | TrainLoss: 0.0297\n",
      "[2017-12-02 18:02:48,296 INFO] Epoch: 11 | ValAcc: 0.7700 | ValLoss: 0.0745 \n",
      "\n",
      "[2017-12-02 18:02:49,066 INFO] Epoch: 12 | TrainAcc: 0.8870 | TrainLoss: 0.0334\n",
      "[2017-12-02 18:02:49,441 INFO] Epoch: 12 | ValAcc: 0.8000 | ValLoss: 0.0682 \n",
      "\n",
      "[2017-12-02 18:02:50,206 INFO] Epoch: 13 | TrainAcc: 0.9320 | TrainLoss: 0.0226\n",
      "[2017-12-02 18:02:50,551 INFO] Epoch: 13 | ValAcc: 0.8000 | ValLoss: 0.0668 \n",
      "\n",
      "[2017-12-02 18:02:51,258 INFO] Epoch: 14 | TrainAcc: 0.9520 | TrainLoss: 0.0190\n",
      "[2017-12-02 18:02:51,601 INFO] Epoch: 14 | ValAcc: 0.8570 | ValLoss: 0.0488 \n",
      "\n",
      "[2017-12-02 18:02:52,302 INFO] Epoch: 15 | TrainAcc: 0.9520 | TrainLoss: 0.0143\n",
      "[2017-12-02 18:02:52,658 INFO] Epoch: 15 | ValAcc: 0.8320 | ValLoss: 0.0593 \n",
      "\n",
      "[2017-12-02 18:02:53,387 INFO] Epoch: 16 | TrainAcc: 0.9590 | TrainLoss: 0.0137\n",
      "[2017-12-02 18:02:53,735 INFO] Epoch: 16 | ValAcc: 0.8410 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:02:54,453 INFO] Epoch: 17 | TrainAcc: 0.9590 | TrainLoss: 0.0124\n",
      "[2017-12-02 18:02:54,807 INFO] Epoch: 17 | ValAcc: 0.8620 | ValLoss: 0.0470 \n",
      "\n",
      "[2017-12-02 18:02:55,569 INFO] Epoch: 18 | TrainAcc: 0.9810 | TrainLoss: 0.0078\n",
      "[2017-12-02 18:02:55,971 INFO] Epoch: 18 | ValAcc: 0.8590 | ValLoss: 0.0447 \n",
      "\n",
      "[2017-12-02 18:02:56,752 INFO] Epoch: 19 | TrainAcc: 0.9680 | TrainLoss: 0.0112\n",
      "[2017-12-02 18:02:57,156 INFO] Epoch: 19 | ValAcc: 0.8340 | ValLoss: 0.0592 \n",
      "\n",
      "[2017-12-02 18:02:57,974 INFO] Epoch: 20 | TrainAcc: 0.9600 | TrainLoss: 0.0138\n",
      "[2017-12-02 18:02:58,341 INFO] Epoch: 20 | ValAcc: 0.7970 | ValLoss: 0.0780 \n",
      "\n",
      "[2017-12-02 18:02:59,092 INFO] Epoch: 21 | TrainAcc: 0.9450 | TrainLoss: 0.0153\n",
      "[2017-12-02 18:02:59,499 INFO] Epoch: 21 | ValAcc: 0.8430 | ValLoss: 0.0559 \n",
      "\n",
      "[2017-12-02 18:03:00,301 INFO] Epoch: 22 | TrainAcc: 0.9670 | TrainLoss: 0.0113\n",
      "[2017-12-02 18:03:00,724 INFO] Epoch: 22 | ValAcc: 0.8610 | ValLoss: 0.0481 \n",
      "\n",
      "[2017-12-02 18:03:01,558 INFO] Epoch: 23 | TrainAcc: 0.9840 | TrainLoss: 0.0067\n",
      "[2017-12-02 18:03:01,969 INFO] Epoch: 23 | ValAcc: 0.8710 | ValLoss: 0.0431 \n",
      "\n",
      "[2017-12-02 18:03:02,825 INFO] Epoch: 24 | TrainAcc: 0.9890 | TrainLoss: 0.0048\n",
      "[2017-12-02 18:03:03,282 INFO] Epoch: 24 | ValAcc: 0.8620 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:03:04,034 INFO] Epoch: 25 | TrainAcc: 0.9870 | TrainLoss: 0.0045\n",
      "[2017-12-02 18:03:04,399 INFO] Epoch: 25 | ValAcc: 0.8550 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:03:05,163 INFO] Epoch: 26 | TrainAcc: 0.9950 | TrainLoss: 0.0033\n",
      "[2017-12-02 18:03:05,532 INFO] Epoch: 26 | ValAcc: 0.8610 | ValLoss: 0.0479 \n",
      "\n",
      "[2017-12-02 18:03:06,265 INFO] Epoch: 27 | TrainAcc: 0.9970 | TrainLoss: 0.0022\n",
      "[2017-12-02 18:03:06,632 INFO] Epoch: 27 | ValAcc: 0.8870 | ValLoss: 0.0451 \n",
      "\n",
      "[2017-12-02 18:03:07,374 INFO] Epoch: 28 | TrainAcc: 1.0000 | TrainLoss: 0.0012\n",
      "[2017-12-02 18:03:07,737 INFO] Epoch: 28 | ValAcc: 0.8970 | ValLoss: 0.0410 \n",
      "\n",
      "[2017-12-02 18:03:08,451 INFO] Epoch: 29 | TrainAcc: 1.0000 | TrainLoss: 0.0009\n",
      "[2017-12-02 18:03:08,819 INFO] Epoch: 29 | ValAcc: 0.8910 | ValLoss: 0.0420 \n",
      "\n",
      "[2017-12-02 18:03:09,551 INFO] Epoch: 30 | TrainAcc: 1.0000 | TrainLoss: 0.0007\n",
      "[2017-12-02 18:03:09,941 INFO] Epoch: 30 | ValAcc: 0.8880 | ValLoss: 0.0437 \n",
      "\n",
      "[2017-12-02 18:03:10,697 INFO] Epoch: 31 | TrainAcc: 1.0000 | TrainLoss: 0.0006\n",
      "[2017-12-02 18:03:11,065 INFO] Epoch: 31 | ValAcc: 0.9020 | ValLoss: 0.0403 \n",
      "\n",
      "[2017-12-02 18:03:11,824 INFO] Epoch: 32 | TrainAcc: 1.0000 | TrainLoss: 0.0005\n",
      "[2017-12-02 18:03:12,189 INFO] Epoch: 32 | ValAcc: 0.8920 | ValLoss: 0.0419 \n",
      "\n",
      "[2017-12-02 18:03:12,922 INFO] Epoch: 33 | TrainAcc: 1.0000 | TrainLoss: 0.0005\n",
      "[2017-12-02 18:03:13,273 INFO] Epoch: 33 | ValAcc: 0.8950 | ValLoss: 0.0415 \n",
      "\n",
      "[2017-12-02 18:03:13,985 INFO] Epoch: 34 | TrainAcc: 1.0000 | TrainLoss: 0.0004\n",
      "[2017-12-02 18:03:14,347 INFO] Epoch: 34 | ValAcc: 0.9000 | ValLoss: 0.0409 \n",
      "\n",
      "[2017-12-02 18:03:15,102 INFO] Epoch: 35 | TrainAcc: 1.0000 | TrainLoss: 0.0004\n",
      "[2017-12-02 18:03:15,490 INFO] Epoch: 35 | ValAcc: 0.9010 | ValLoss: 0.0415 \n",
      "\n",
      "[2017-12-02 18:03:16,201 INFO] Epoch: 36 | TrainAcc: 1.0000 | TrainLoss: 0.0004\n",
      "[2017-12-02 18:03:16,564 INFO] Epoch: 36 | ValAcc: 0.9000 | ValLoss: 0.0412 \n",
      "\n",
      "[2017-12-02 18:03:17,285 INFO] Epoch: 37 | TrainAcc: 1.0000 | TrainLoss: 0.0003\n",
      "[2017-12-02 18:03:17,660 INFO] Epoch: 37 | ValAcc: 0.8980 | ValLoss: 0.0416 \n",
      "\n",
      "[2017-12-02 18:03:18,400 INFO] Epoch: 38 | TrainAcc: 1.0000 | TrainLoss: 0.0003\n",
      "[2017-12-02 18:03:18,757 INFO] Epoch: 38 | ValAcc: 0.9010 | ValLoss: 0.0419 \n",
      "\n",
      "[2017-12-02 18:03:19,484 INFO] Epoch: 39 | TrainAcc: 1.0000 | TrainLoss: 0.0003\n",
      "[2017-12-02 18:03:19,868 INFO] Epoch: 39 | ValAcc: 0.9030 | ValLoss: 0.0415 \n",
      "\n",
      "[2017-12-02 18:03:20,603 INFO] Epoch: 40 | TrainAcc: 1.0000 | TrainLoss: 0.0003\n",
      "[2017-12-02 18:03:20,964 INFO] Epoch: 40 | ValAcc: 0.9000 | ValLoss: 0.0422 \n",
      "\n",
      "[2017-12-02 18:03:21,715 INFO] Epoch: 41 | TrainAcc: 1.0000 | TrainLoss: 0.0003\n",
      "[2017-12-02 18:03:22,151 INFO] Epoch: 41 | ValAcc: 0.9010 | ValLoss: 0.0422 \n",
      "\n",
      "[2017-12-02 18:03:22,902 INFO] Epoch: 42 | TrainAcc: 1.0000 | TrainLoss: 0.0003\n",
      "[2017-12-02 18:03:23,308 INFO] Epoch: 42 | ValAcc: 0.9010 | ValLoss: 0.0419 \n",
      "\n",
      "[2017-12-02 18:03:24,033 INFO] Epoch: 43 | TrainAcc: 1.0000 | TrainLoss: 0.0003\n",
      "[2017-12-02 18:03:24,410 INFO] Epoch: 43 | ValAcc: 0.9010 | ValLoss: 0.0422 \n",
      "\n",
      "[2017-12-02 18:03:25,150 INFO] Epoch: 44 | TrainAcc: 1.0000 | TrainLoss: 0.0003\n",
      "[2017-12-02 18:03:25,523 INFO] Epoch: 44 | ValAcc: 0.8970 | ValLoss: 0.0430 \n",
      "\n",
      "[2017-12-02 18:03:26,287 INFO] Epoch: 45 | TrainAcc: 1.0000 | TrainLoss: 0.0003\n",
      "[2017-12-02 18:03:26,658 INFO] Epoch: 45 | ValAcc: 0.9000 | ValLoss: 0.0423 \n",
      "\n",
      "[2017-12-02 18:03:27,385 INFO] Epoch: 46 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:27,755 INFO] Epoch: 46 | ValAcc: 0.8990 | ValLoss: 0.0424 \n",
      "\n",
      "[2017-12-02 18:03:28,480 INFO] Epoch: 47 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:28,843 INFO] Epoch: 47 | ValAcc: 0.8990 | ValLoss: 0.0426 \n",
      "\n",
      "[2017-12-02 18:03:29,574 INFO] Epoch: 48 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:29,966 INFO] Epoch: 48 | ValAcc: 0.8990 | ValLoss: 0.0429 \n",
      "\n",
      "[2017-12-02 18:03:30,805 INFO] Epoch: 49 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:31,161 INFO] Epoch: 49 | ValAcc: 0.8990 | ValLoss: 0.0427 \n",
      "\n",
      "[2017-12-02 18:03:31,888 INFO] Epoch: 50 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:32,234 INFO] Epoch: 50 | ValAcc: 0.8950 | ValLoss: 0.0435 \n",
      "\n",
      "[2017-12-02 18:03:32,958 INFO] Epoch: 51 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:33,334 INFO] Epoch: 51 | ValAcc: 0.8960 | ValLoss: 0.0430 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:03:34,088 INFO] Epoch: 52 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:34,417 INFO] Epoch: 52 | ValAcc: 0.8960 | ValLoss: 0.0430 \n",
      "\n",
      "[2017-12-02 18:03:35,072 INFO] Epoch: 53 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:35,405 INFO] Epoch: 53 | ValAcc: 0.8950 | ValLoss: 0.0432 \n",
      "\n",
      "[2017-12-02 18:03:36,052 INFO] Epoch: 54 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:36,424 INFO] Epoch: 54 | ValAcc: 0.8970 | ValLoss: 0.0434 \n",
      "\n",
      "[2017-12-02 18:03:37,091 INFO] Epoch: 55 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:37,417 INFO] Epoch: 55 | ValAcc: 0.8950 | ValLoss: 0.0431 \n",
      "\n",
      "[2017-12-02 18:03:38,072 INFO] Epoch: 56 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:38,399 INFO] Epoch: 56 | ValAcc: 0.8970 | ValLoss: 0.0436 \n",
      "\n",
      "[2017-12-02 18:03:39,053 INFO] Epoch: 57 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:39,386 INFO] Epoch: 57 | ValAcc: 0.8960 | ValLoss: 0.0439 \n",
      "\n",
      "[2017-12-02 18:03:40,061 INFO] Epoch: 58 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:40,384 INFO] Epoch: 58 | ValAcc: 0.8980 | ValLoss: 0.0437 \n",
      "\n",
      "[2017-12-02 18:03:41,042 INFO] Epoch: 59 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:41,366 INFO] Epoch: 59 | ValAcc: 0.8950 | ValLoss: 0.0436 \n",
      "\n",
      "[2017-12-02 18:03:42,039 INFO] Epoch: 60 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:42,369 INFO] Epoch: 60 | ValAcc: 0.8970 | ValLoss: 0.0437 \n",
      "\n",
      "[2017-12-02 18:03:43,038 INFO] Epoch: 61 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:43,361 INFO] Epoch: 61 | ValAcc: 0.8940 | ValLoss: 0.0438 \n",
      "\n",
      "[2017-12-02 18:03:44,024 INFO] Epoch: 62 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:44,351 INFO] Epoch: 62 | ValAcc: 0.8970 | ValLoss: 0.0437 \n",
      "\n",
      "[2017-12-02 18:03:45,024 INFO] Epoch: 63 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:45,344 INFO] Epoch: 63 | ValAcc: 0.8940 | ValLoss: 0.0441 \n",
      "\n",
      "[2017-12-02 18:03:46,003 INFO] Epoch: 64 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:46,322 INFO] Epoch: 64 | ValAcc: 0.8960 | ValLoss: 0.0434 \n",
      "\n",
      "[2017-12-02 18:03:46,984 INFO] Epoch: 65 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:47,306 INFO] Epoch: 65 | ValAcc: 0.8960 | ValLoss: 0.0439 \n",
      "\n",
      "[2017-12-02 18:03:47,966 INFO] Epoch: 66 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:48,292 INFO] Epoch: 66 | ValAcc: 0.8960 | ValLoss: 0.0442 \n",
      "\n",
      "[2017-12-02 18:03:48,953 INFO] Epoch: 67 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:03:49,276 INFO] Epoch: 67 | ValAcc: 0.8950 | ValLoss: 0.0439 \n",
      "\n",
      "[2017-12-02 18:03:49,931 INFO] Epoch: 68 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:03:50,293 INFO] Epoch: 68 | ValAcc: 0.8970 | ValLoss: 0.0439 \n",
      "\n",
      "[2017-12-02 18:03:50,947 INFO] Epoch: 69 | TrainAcc: 1.0000 | TrainLoss: 0.0002\n",
      "[2017-12-02 18:03:51,265 INFO] Epoch: 69 | ValAcc: 0.8950 | ValLoss: 0.0442 \n",
      "\n",
      "[2017-12-02 18:03:51,935 INFO] Epoch: 70 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:03:52,263 INFO] Epoch: 70 | ValAcc: 0.8970 | ValLoss: 0.0442 \n",
      "\n",
      "[2017-12-02 18:03:52,915 INFO] Epoch: 71 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:03:53,268 INFO] Epoch: 71 | ValAcc: 0.8950 | ValLoss: 0.0442 \n",
      "\n",
      "[2017-12-02 18:03:53,923 INFO] Epoch: 72 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:03:54,242 INFO] Epoch: 72 | ValAcc: 0.8950 | ValLoss: 0.0444 \n",
      "\n",
      "[2017-12-02 18:03:54,891 INFO] Epoch: 73 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:03:55,247 INFO] Epoch: 73 | ValAcc: 0.8960 | ValLoss: 0.0443 \n",
      "\n",
      "[2017-12-02 18:03:55,900 INFO] Epoch: 74 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:03:56,217 INFO] Epoch: 74 | ValAcc: 0.8960 | ValLoss: 0.0442 \n",
      "\n",
      "[2017-12-02 18:03:56,877 INFO] Epoch: 75 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:03:57,194 INFO] Epoch: 75 | ValAcc: 0.8970 | ValLoss: 0.0445 \n",
      "\n",
      "[2017-12-02 18:03:57,856 INFO] Epoch: 76 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:03:58,174 INFO] Epoch: 76 | ValAcc: 0.8940 | ValLoss: 0.0445 \n",
      "\n",
      "[2017-12-02 18:03:58,833 INFO] Epoch: 77 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:03:59,158 INFO] Epoch: 77 | ValAcc: 0.8950 | ValLoss: 0.0447 \n",
      "\n",
      "[2017-12-02 18:03:59,822 INFO] Epoch: 78 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:00,150 INFO] Epoch: 78 | ValAcc: 0.8950 | ValLoss: 0.0446 \n",
      "\n",
      "[2017-12-02 18:04:00,820 INFO] Epoch: 79 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:01,148 INFO] Epoch: 79 | ValAcc: 0.8970 | ValLoss: 0.0446 \n",
      "\n",
      "[2017-12-02 18:04:01,815 INFO] Epoch: 80 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:02,138 INFO] Epoch: 80 | ValAcc: 0.8950 | ValLoss: 0.0446 \n",
      "\n",
      "[2017-12-02 18:04:02,793 INFO] Epoch: 81 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:03,118 INFO] Epoch: 81 | ValAcc: 0.8960 | ValLoss: 0.0446 \n",
      "\n",
      "[2017-12-02 18:04:03,793 INFO] Epoch: 82 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:04,116 INFO] Epoch: 82 | ValAcc: 0.8950 | ValLoss: 0.0448 \n",
      "\n",
      "[2017-12-02 18:04:04,771 INFO] Epoch: 83 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:05,092 INFO] Epoch: 83 | ValAcc: 0.8970 | ValLoss: 0.0447 \n",
      "\n",
      "[2017-12-02 18:04:05,754 INFO] Epoch: 84 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:06,086 INFO] Epoch: 84 | ValAcc: 0.8980 | ValLoss: 0.0446 \n",
      "\n",
      "[2017-12-02 18:04:06,758 INFO] Epoch: 85 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:07,077 INFO] Epoch: 85 | ValAcc: 0.8950 | ValLoss: 0.0447 \n",
      "\n",
      "[2017-12-02 18:04:07,735 INFO] Epoch: 86 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:08,057 INFO] Epoch: 86 | ValAcc: 0.8920 | ValLoss: 0.0453 \n",
      "\n",
      "[2017-12-02 18:04:08,709 INFO] Epoch: 87 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:09,039 INFO] Epoch: 87 | ValAcc: 0.8950 | ValLoss: 0.0451 \n",
      "\n",
      "[2017-12-02 18:04:09,704 INFO] Epoch: 88 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:10,022 INFO] Epoch: 88 | ValAcc: 0.8970 | ValLoss: 0.0450 \n",
      "\n",
      "[2017-12-02 18:04:10,687 INFO] Epoch: 89 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:11,007 INFO] Epoch: 89 | ValAcc: 0.8980 | ValLoss: 0.0448 \n",
      "\n",
      "[2017-12-02 18:04:11,675 INFO] Epoch: 90 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:12,015 INFO] Epoch: 90 | ValAcc: 0.8960 | ValLoss: 0.0451 \n",
      "\n",
      "[2017-12-02 18:04:12,686 INFO] Epoch: 91 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:13,007 INFO] Epoch: 91 | ValAcc: 0.8960 | ValLoss: 0.0448 \n",
      "\n",
      "[2017-12-02 18:04:13,666 INFO] Epoch: 92 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:13,989 INFO] Epoch: 92 | ValAcc: 0.8980 | ValLoss: 0.0452 \n",
      "\n",
      "[2017-12-02 18:04:14,644 INFO] Epoch: 93 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:14,966 INFO] Epoch: 93 | ValAcc: 0.8950 | ValLoss: 0.0456 \n",
      "\n",
      "[2017-12-02 18:04:15,623 INFO] Epoch: 94 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:15,943 INFO] Epoch: 94 | ValAcc: 0.8970 | ValLoss: 0.0454 \n",
      "\n",
      "[2017-12-02 18:04:16,607 INFO] Epoch: 95 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:16,932 INFO] Epoch: 95 | ValAcc: 0.8970 | ValLoss: 0.0452 \n",
      "\n",
      "[2017-12-02 18:04:17,588 INFO] Epoch: 96 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:17,913 INFO] Epoch: 96 | ValAcc: 0.8960 | ValLoss: 0.0452 \n",
      "\n",
      "[2017-12-02 18:04:18,568 INFO] Epoch: 97 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:18,893 INFO] Epoch: 97 | ValAcc: 0.8960 | ValLoss: 0.0451 \n",
      "\n",
      "[2017-12-02 18:04:19,561 INFO] Epoch: 98 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:19,884 INFO] Epoch: 98 | ValAcc: 0.8980 | ValLoss: 0.0453 \n",
      "\n",
      "[2017-12-02 18:04:20,553 INFO] Epoch: 99 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:20,878 INFO] Epoch: 99 | ValAcc: 0.8970 | ValLoss: 0.0454 \n",
      "\n",
      "[2017-12-02 18:04:21,532 INFO] Epoch: 100 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:21,850 INFO] Epoch: 100 | ValAcc: 0.8980 | ValLoss: 0.0452 \n",
      "\n",
      "[2017-12-02 18:04:22,498 INFO] Epoch: 101 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:22,838 INFO] Epoch: 101 | ValAcc: 0.8970 | ValLoss: 0.0454 \n",
      "\n",
      "[2017-12-02 18:04:23,508 INFO] Epoch: 102 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:23,836 INFO] Epoch: 102 | ValAcc: 0.8960 | ValLoss: 0.0456 \n",
      "\n",
      "[2017-12-02 18:04:24,504 INFO] Epoch: 103 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:24,824 INFO] Epoch: 103 | ValAcc: 0.8960 | ValLoss: 0.0454 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:04:25,493 INFO] Epoch: 104 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:25,814 INFO] Epoch: 104 | ValAcc: 0.8970 | ValLoss: 0.0456 \n",
      "\n",
      "[2017-12-02 18:04:26,477 INFO] Epoch: 105 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:26,798 INFO] Epoch: 105 | ValAcc: 0.8970 | ValLoss: 0.0454 \n",
      "\n",
      "[2017-12-02 18:04:27,459 INFO] Epoch: 106 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:27,782 INFO] Epoch: 106 | ValAcc: 0.8970 | ValLoss: 0.0460 \n",
      "\n",
      "[2017-12-02 18:04:28,444 INFO] Epoch: 107 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:28,764 INFO] Epoch: 107 | ValAcc: 0.8980 | ValLoss: 0.0458 \n",
      "\n",
      "[2017-12-02 18:04:29,421 INFO] Epoch: 108 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:29,743 INFO] Epoch: 108 | ValAcc: 0.8990 | ValLoss: 0.0457 \n",
      "\n",
      "[2017-12-02 18:04:30,409 INFO] Epoch: 109 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:30,744 INFO] Epoch: 109 | ValAcc: 0.8960 | ValLoss: 0.0457 \n",
      "\n",
      "[2017-12-02 18:04:31,396 INFO] Epoch: 110 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:31,726 INFO] Epoch: 110 | ValAcc: 0.8980 | ValLoss: 0.0458 \n",
      "\n",
      "[2017-12-02 18:04:32,390 INFO] Epoch: 111 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:32,709 INFO] Epoch: 111 | ValAcc: 0.8950 | ValLoss: 0.0459 \n",
      "\n",
      "[2017-12-02 18:04:33,355 INFO] Epoch: 112 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:33,693 INFO] Epoch: 112 | ValAcc: 0.8970 | ValLoss: 0.0454 \n",
      "\n",
      "[2017-12-02 18:04:34,350 INFO] Epoch: 113 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:34,670 INFO] Epoch: 113 | ValAcc: 0.8980 | ValLoss: 0.0457 \n",
      "\n",
      "[2017-12-02 18:04:35,334 INFO] Epoch: 114 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:35,657 INFO] Epoch: 114 | ValAcc: 0.8980 | ValLoss: 0.0458 \n",
      "\n",
      "[2017-12-02 18:04:36,312 INFO] Epoch: 115 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:36,640 INFO] Epoch: 115 | ValAcc: 0.8990 | ValLoss: 0.0461 \n",
      "\n",
      "[2017-12-02 18:04:37,285 INFO] Epoch: 116 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:37,605 INFO] Epoch: 116 | ValAcc: 0.8980 | ValLoss: 0.0460 \n",
      "\n",
      "[2017-12-02 18:04:38,259 INFO] Epoch: 117 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:38,579 INFO] Epoch: 117 | ValAcc: 0.8980 | ValLoss: 0.0458 \n",
      "\n",
      "[2017-12-02 18:04:39,232 INFO] Epoch: 118 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:39,555 INFO] Epoch: 118 | ValAcc: 0.8980 | ValLoss: 0.0459 \n",
      "\n",
      "[2017-12-02 18:04:40,219 INFO] Epoch: 119 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:40,548 INFO] Epoch: 119 | ValAcc: 0.8970 | ValLoss: 0.0461 \n",
      "\n",
      "[2017-12-02 18:04:41,210 INFO] Epoch: 120 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:41,562 INFO] Epoch: 120 | ValAcc: 0.8970 | ValLoss: 0.0463 \n",
      "\n",
      "[2017-12-02 18:04:42,218 INFO] Epoch: 121 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:42,547 INFO] Epoch: 121 | ValAcc: 0.8970 | ValLoss: 0.0461 \n",
      "\n",
      "[2017-12-02 18:04:43,199 INFO] Epoch: 122 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:43,523 INFO] Epoch: 122 | ValAcc: 0.8970 | ValLoss: 0.0459 \n",
      "\n",
      "[2017-12-02 18:04:44,184 INFO] Epoch: 123 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:44,540 INFO] Epoch: 123 | ValAcc: 0.8980 | ValLoss: 0.0462 \n",
      "\n",
      "[2017-12-02 18:04:45,199 INFO] Epoch: 124 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:45,528 INFO] Epoch: 124 | ValAcc: 0.8980 | ValLoss: 0.0463 \n",
      "\n",
      "[2017-12-02 18:04:46,185 INFO] Epoch: 125 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:46,504 INFO] Epoch: 125 | ValAcc: 0.8970 | ValLoss: 0.0463 \n",
      "\n",
      "[2017-12-02 18:04:47,168 INFO] Epoch: 126 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:47,489 INFO] Epoch: 126 | ValAcc: 0.8970 | ValLoss: 0.0462 \n",
      "\n",
      "[2017-12-02 18:04:48,136 INFO] Epoch: 127 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:48,459 INFO] Epoch: 127 | ValAcc: 0.8970 | ValLoss: 0.0463 \n",
      "\n",
      "[2017-12-02 18:04:49,115 INFO] Epoch: 128 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:49,437 INFO] Epoch: 128 | ValAcc: 0.8980 | ValLoss: 0.0463 \n",
      "\n",
      "[2017-12-02 18:04:50,094 INFO] Epoch: 129 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:50,415 INFO] Epoch: 129 | ValAcc: 0.8970 | ValLoss: 0.0464 \n",
      "\n",
      "[2017-12-02 18:04:51,078 INFO] Epoch: 130 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:51,403 INFO] Epoch: 130 | ValAcc: 0.8970 | ValLoss: 0.0463 \n",
      "\n",
      "[2017-12-02 18:04:52,059 INFO] Epoch: 131 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:52,378 INFO] Epoch: 131 | ValAcc: 0.8980 | ValLoss: 0.0462 \n",
      "\n",
      "[2017-12-02 18:04:53,035 INFO] Epoch: 132 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:53,366 INFO] Epoch: 132 | ValAcc: 0.8950 | ValLoss: 0.0464 \n",
      "\n",
      "[2017-12-02 18:04:54,021 INFO] Epoch: 133 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:54,338 INFO] Epoch: 133 | ValAcc: 0.8960 | ValLoss: 0.0464 \n",
      "\n",
      "[2017-12-02 18:04:54,986 INFO] Epoch: 134 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:55,316 INFO] Epoch: 134 | ValAcc: 0.8980 | ValLoss: 0.0464 \n",
      "\n",
      "[2017-12-02 18:04:55,973 INFO] Epoch: 135 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:56,297 INFO] Epoch: 135 | ValAcc: 0.8980 | ValLoss: 0.0464 \n",
      "\n",
      "[2017-12-02 18:04:56,956 INFO] Epoch: 136 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:57,272 INFO] Epoch: 136 | ValAcc: 0.8980 | ValLoss: 0.0463 \n",
      "\n",
      "[2017-12-02 18:04:57,922 INFO] Epoch: 137 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:58,249 INFO] Epoch: 137 | ValAcc: 0.9000 | ValLoss: 0.0464 \n",
      "\n",
      "[2017-12-02 18:04:58,896 INFO] Epoch: 138 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:04:59,216 INFO] Epoch: 138 | ValAcc: 0.8990 | ValLoss: 0.0465 \n",
      "\n",
      "[2017-12-02 18:04:59,875 INFO] Epoch: 139 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:00,197 INFO] Epoch: 139 | ValAcc: 0.8970 | ValLoss: 0.0467 \n",
      "\n",
      "[2017-12-02 18:05:00,867 INFO] Epoch: 140 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:01,188 INFO] Epoch: 140 | ValAcc: 0.8970 | ValLoss: 0.0464 \n",
      "\n",
      "[2017-12-02 18:05:01,835 INFO] Epoch: 141 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:02,150 INFO] Epoch: 141 | ValAcc: 0.8960 | ValLoss: 0.0464 \n",
      "\n",
      "[2017-12-02 18:05:02,808 INFO] Epoch: 142 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:03,137 INFO] Epoch: 142 | ValAcc: 0.8970 | ValLoss: 0.0466 \n",
      "\n",
      "[2017-12-02 18:05:03,794 INFO] Epoch: 143 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:04,126 INFO] Epoch: 143 | ValAcc: 0.8970 | ValLoss: 0.0467 \n",
      "\n",
      "[2017-12-02 18:05:04,781 INFO] Epoch: 144 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:05,104 INFO] Epoch: 144 | ValAcc: 0.8980 | ValLoss: 0.0465 \n",
      "\n",
      "[2017-12-02 18:05:05,771 INFO] Epoch: 145 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:06,096 INFO] Epoch: 145 | ValAcc: 0.8980 | ValLoss: 0.0467 \n",
      "\n",
      "[2017-12-02 18:05:06,751 INFO] Epoch: 146 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:07,071 INFO] Epoch: 146 | ValAcc: 0.8970 | ValLoss: 0.0469 \n",
      "\n",
      "[2017-12-02 18:05:07,723 INFO] Epoch: 147 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:08,047 INFO] Epoch: 147 | ValAcc: 0.8980 | ValLoss: 0.0467 \n",
      "\n",
      "[2017-12-02 18:05:08,712 INFO] Epoch: 148 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:09,041 INFO] Epoch: 148 | ValAcc: 0.8960 | ValLoss: 0.0467 \n",
      "\n",
      "[2017-12-02 18:05:09,713 INFO] Epoch: 149 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:10,035 INFO] Epoch: 149 | ValAcc: 0.8960 | ValLoss: 0.0466 \n",
      "\n",
      "[2017-12-02 18:05:10,697 INFO] Epoch: 150 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:11,030 INFO] Epoch: 150 | ValAcc: 0.8970 | ValLoss: 0.0466 \n",
      "\n",
      "[2017-12-02 18:05:11,705 INFO] Epoch: 151 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:12,042 INFO] Epoch: 151 | ValAcc: 0.8980 | ValLoss: 0.0468 \n",
      "\n",
      "[2017-12-02 18:05:12,701 INFO] Epoch: 152 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:13,018 INFO] Epoch: 152 | ValAcc: 0.8970 | ValLoss: 0.0470 \n",
      "\n",
      "[2017-12-02 18:05:13,666 INFO] Epoch: 153 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:13,987 INFO] Epoch: 153 | ValAcc: 0.8960 | ValLoss: 0.0470 \n",
      "\n",
      "[2017-12-02 18:05:14,652 INFO] Epoch: 154 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:14,986 INFO] Epoch: 154 | ValAcc: 0.8970 | ValLoss: 0.0469 \n",
      "\n",
      "[2017-12-02 18:05:15,644 INFO] Epoch: 155 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:05:15,964 INFO] Epoch: 155 | ValAcc: 0.8970 | ValLoss: 0.0468 \n",
      "\n",
      "[2017-12-02 18:05:16,630 INFO] Epoch: 156 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:16,949 INFO] Epoch: 156 | ValAcc: 0.8970 | ValLoss: 0.0469 \n",
      "\n",
      "[2017-12-02 18:05:17,603 INFO] Epoch: 157 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:17,922 INFO] Epoch: 157 | ValAcc: 0.8970 | ValLoss: 0.0470 \n",
      "\n",
      "[2017-12-02 18:05:18,578 INFO] Epoch: 158 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:18,902 INFO] Epoch: 158 | ValAcc: 0.8980 | ValLoss: 0.0470 \n",
      "\n",
      "[2017-12-02 18:05:19,557 INFO] Epoch: 159 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:19,883 INFO] Epoch: 159 | ValAcc: 0.8980 | ValLoss: 0.0470 \n",
      "\n",
      "[2017-12-02 18:05:20,534 INFO] Epoch: 160 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:20,854 INFO] Epoch: 160 | ValAcc: 0.8970 | ValLoss: 0.0469 \n",
      "\n",
      "[2017-12-02 18:05:21,517 INFO] Epoch: 161 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:21,836 INFO] Epoch: 161 | ValAcc: 0.8970 | ValLoss: 0.0469 \n",
      "\n",
      "[2017-12-02 18:05:22,486 INFO] Epoch: 162 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:22,813 INFO] Epoch: 162 | ValAcc: 0.8970 | ValLoss: 0.0469 \n",
      "\n",
      "[2017-12-02 18:05:23,460 INFO] Epoch: 163 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:23,785 INFO] Epoch: 163 | ValAcc: 0.8980 | ValLoss: 0.0469 \n",
      "\n",
      "[2017-12-02 18:05:24,451 INFO] Epoch: 164 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:24,771 INFO] Epoch: 164 | ValAcc: 0.8960 | ValLoss: 0.0472 \n",
      "\n",
      "[2017-12-02 18:05:25,429 INFO] Epoch: 165 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:25,770 INFO] Epoch: 165 | ValAcc: 0.8980 | ValLoss: 0.0469 \n",
      "\n",
      "[2017-12-02 18:05:26,422 INFO] Epoch: 166 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:26,742 INFO] Epoch: 166 | ValAcc: 0.8970 | ValLoss: 0.0469 \n",
      "\n",
      "[2017-12-02 18:05:27,395 INFO] Epoch: 167 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:27,717 INFO] Epoch: 167 | ValAcc: 0.8970 | ValLoss: 0.0468 \n",
      "\n",
      "[2017-12-02 18:05:28,367 INFO] Epoch: 168 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:28,691 INFO] Epoch: 168 | ValAcc: 0.8970 | ValLoss: 0.0469 \n",
      "\n",
      "[2017-12-02 18:05:29,345 INFO] Epoch: 169 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:29,669 INFO] Epoch: 169 | ValAcc: 0.8970 | ValLoss: 0.0471 \n",
      "\n",
      "[2017-12-02 18:05:30,328 INFO] Epoch: 170 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:30,649 INFO] Epoch: 170 | ValAcc: 0.8970 | ValLoss: 0.0472 \n",
      "\n",
      "[2017-12-02 18:05:31,315 INFO] Epoch: 171 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:31,641 INFO] Epoch: 171 | ValAcc: 0.8970 | ValLoss: 0.0472 \n",
      "\n",
      "[2017-12-02 18:05:32,293 INFO] Epoch: 172 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:32,614 INFO] Epoch: 172 | ValAcc: 0.8970 | ValLoss: 0.0472 \n",
      "\n",
      "[2017-12-02 18:05:33,269 INFO] Epoch: 173 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:33,591 INFO] Epoch: 173 | ValAcc: 0.8970 | ValLoss: 0.0471 \n",
      "\n",
      "[2017-12-02 18:05:34,250 INFO] Epoch: 174 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:34,581 INFO] Epoch: 174 | ValAcc: 0.8980 | ValLoss: 0.0473 \n",
      "\n",
      "[2017-12-02 18:05:35,228 INFO] Epoch: 175 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:35,548 INFO] Epoch: 175 | ValAcc: 0.8970 | ValLoss: 0.0472 \n",
      "\n",
      "[2017-12-02 18:05:36,201 INFO] Epoch: 176 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:36,529 INFO] Epoch: 176 | ValAcc: 0.8980 | ValLoss: 0.0473 \n",
      "\n",
      "[2017-12-02 18:05:37,183 INFO] Epoch: 177 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:37,514 INFO] Epoch: 177 | ValAcc: 0.8980 | ValLoss: 0.0473 \n",
      "\n",
      "[2017-12-02 18:05:38,169 INFO] Epoch: 178 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:38,518 INFO] Epoch: 178 | ValAcc: 0.8970 | ValLoss: 0.0473 \n",
      "\n",
      "[2017-12-02 18:05:39,205 INFO] Epoch: 179 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:39,544 INFO] Epoch: 179 | ValAcc: 0.8970 | ValLoss: 0.0473 \n",
      "\n",
      "[2017-12-02 18:05:40,304 INFO] Epoch: 180 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:40,666 INFO] Epoch: 180 | ValAcc: 0.8980 | ValLoss: 0.0473 \n",
      "\n",
      "[2017-12-02 18:05:41,372 INFO] Epoch: 181 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:41,713 INFO] Epoch: 181 | ValAcc: 0.8970 | ValLoss: 0.0473 \n",
      "\n",
      "[2017-12-02 18:05:42,423 INFO] Epoch: 182 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:42,767 INFO] Epoch: 182 | ValAcc: 0.8970 | ValLoss: 0.0474 \n",
      "\n",
      "[2017-12-02 18:05:43,480 INFO] Epoch: 183 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:43,818 INFO] Epoch: 183 | ValAcc: 0.8970 | ValLoss: 0.0474 \n",
      "\n",
      "[2017-12-02 18:05:44,503 INFO] Epoch: 184 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:44,831 INFO] Epoch: 184 | ValAcc: 0.8980 | ValLoss: 0.0475 \n",
      "\n",
      "[2017-12-02 18:05:45,524 INFO] Epoch: 185 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:45,863 INFO] Epoch: 185 | ValAcc: 0.8970 | ValLoss: 0.0474 \n",
      "\n",
      "[2017-12-02 18:05:46,532 INFO] Epoch: 186 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:46,863 INFO] Epoch: 186 | ValAcc: 0.8970 | ValLoss: 0.0475 \n",
      "\n",
      "[2017-12-02 18:05:47,542 INFO] Epoch: 187 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:47,879 INFO] Epoch: 187 | ValAcc: 0.8970 | ValLoss: 0.0474 \n",
      "\n",
      "[2017-12-02 18:05:48,553 INFO] Epoch: 188 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:48,899 INFO] Epoch: 188 | ValAcc: 0.8970 | ValLoss: 0.0473 \n",
      "\n",
      "[2017-12-02 18:05:49,577 INFO] Epoch: 189 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:49,915 INFO] Epoch: 189 | ValAcc: 0.8960 | ValLoss: 0.0473 \n",
      "\n",
      "[2017-12-02 18:05:50,601 INFO] Epoch: 190 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:50,940 INFO] Epoch: 190 | ValAcc: 0.8970 | ValLoss: 0.0474 \n",
      "\n",
      "[2017-12-02 18:05:51,635 INFO] Epoch: 191 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:51,992 INFO] Epoch: 191 | ValAcc: 0.8960 | ValLoss: 0.0476 \n",
      "\n",
      "[2017-12-02 18:05:52,674 INFO] Epoch: 192 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:52,995 INFO] Epoch: 192 | ValAcc: 0.8960 | ValLoss: 0.0475 \n",
      "\n",
      "[2017-12-02 18:05:53,687 INFO] Epoch: 193 | TrainAcc: 1.0000 | TrainLoss: 0.0001\n",
      "[2017-12-02 18:05:54,012 INFO] Epoch: 193 | ValAcc: 0.8960 | ValLoss: 0.0474 \n",
      "\n",
      "[2017-12-02 18:05:54,678 INFO] Epoch: 194 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:55,001 INFO] Epoch: 194 | ValAcc: 0.8960 | ValLoss: 0.0474 \n",
      "\n",
      "[2017-12-02 18:05:55,677 INFO] Epoch: 195 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:55,998 INFO] Epoch: 195 | ValAcc: 0.8970 | ValLoss: 0.0476 \n",
      "\n",
      "[2017-12-02 18:05:56,665 INFO] Epoch: 196 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:56,988 INFO] Epoch: 196 | ValAcc: 0.8960 | ValLoss: 0.0479 \n",
      "\n",
      "[2017-12-02 18:05:57,654 INFO] Epoch: 197 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:57,980 INFO] Epoch: 197 | ValAcc: 0.8960 | ValLoss: 0.0479 \n",
      "\n",
      "[2017-12-02 18:05:58,646 INFO] Epoch: 198 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:58,965 INFO] Epoch: 198 | ValAcc: 0.8960 | ValLoss: 0.0476 \n",
      "\n",
      "[2017-12-02 18:05:59,629 INFO] Epoch: 199 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:05:59,951 INFO] Epoch: 199 | ValAcc: 0.8970 | ValLoss: 0.0476 \n",
      "\n",
      "[2017-12-02 18:06:00,606 INFO] Epoch: 200 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:00,923 INFO] Epoch: 200 | ValAcc: 0.8970 | ValLoss: 0.0479 \n",
      "\n",
      "[2017-12-02 18:06:01,598 INFO] Epoch: 201 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:01,917 INFO] Epoch: 201 | ValAcc: 0.8970 | ValLoss: 0.0479 \n",
      "\n",
      "[2017-12-02 18:06:02,588 INFO] Epoch: 202 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:02,906 INFO] Epoch: 202 | ValAcc: 0.8970 | ValLoss: 0.0477 \n",
      "\n",
      "[2017-12-02 18:06:03,569 INFO] Epoch: 203 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:03,900 INFO] Epoch: 203 | ValAcc: 0.8970 | ValLoss: 0.0477 \n",
      "\n",
      "[2017-12-02 18:06:04,565 INFO] Epoch: 204 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:04,895 INFO] Epoch: 204 | ValAcc: 0.8970 | ValLoss: 0.0478 \n",
      "\n",
      "[2017-12-02 18:06:05,554 INFO] Epoch: 205 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:05,883 INFO] Epoch: 205 | ValAcc: 0.8970 | ValLoss: 0.0478 \n",
      "\n",
      "[2017-12-02 18:06:06,536 INFO] Epoch: 206 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:06,852 INFO] Epoch: 206 | ValAcc: 0.8970 | ValLoss: 0.0477 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:06:07,511 INFO] Epoch: 207 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:07,835 INFO] Epoch: 207 | ValAcc: 0.8970 | ValLoss: 0.0476 \n",
      "\n",
      "[2017-12-02 18:06:08,486 INFO] Epoch: 208 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:08,815 INFO] Epoch: 208 | ValAcc: 0.8960 | ValLoss: 0.0475 \n",
      "\n",
      "[2017-12-02 18:06:09,473 INFO] Epoch: 209 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:09,796 INFO] Epoch: 209 | ValAcc: 0.8960 | ValLoss: 0.0479 \n",
      "\n",
      "[2017-12-02 18:06:10,455 INFO] Epoch: 210 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:10,776 INFO] Epoch: 210 | ValAcc: 0.8970 | ValLoss: 0.0476 \n",
      "\n",
      "[2017-12-02 18:06:11,441 INFO] Epoch: 211 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:11,788 INFO] Epoch: 211 | ValAcc: 0.8970 | ValLoss: 0.0477 \n",
      "\n",
      "[2017-12-02 18:06:12,452 INFO] Epoch: 212 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:12,768 INFO] Epoch: 212 | ValAcc: 0.8960 | ValLoss: 0.0479 \n",
      "\n",
      "[2017-12-02 18:06:13,425 INFO] Epoch: 213 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:13,745 INFO] Epoch: 213 | ValAcc: 0.8950 | ValLoss: 0.0480 \n",
      "\n",
      "[2017-12-02 18:06:14,400 INFO] Epoch: 214 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:14,725 INFO] Epoch: 214 | ValAcc: 0.8970 | ValLoss: 0.0479 \n",
      "\n",
      "[2017-12-02 18:06:15,398 INFO] Epoch: 215 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:15,733 INFO] Epoch: 215 | ValAcc: 0.8970 | ValLoss: 0.0478 \n",
      "\n",
      "[2017-12-02 18:06:16,415 INFO] Epoch: 216 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:16,751 INFO] Epoch: 216 | ValAcc: 0.8970 | ValLoss: 0.0479 \n",
      "\n",
      "[2017-12-02 18:06:17,443 INFO] Epoch: 217 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:17,824 INFO] Epoch: 217 | ValAcc: 0.8960 | ValLoss: 0.0480 \n",
      "\n",
      "[2017-12-02 18:06:18,569 INFO] Epoch: 218 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:19,019 INFO] Epoch: 218 | ValAcc: 0.8970 | ValLoss: 0.0480 \n",
      "\n",
      "[2017-12-02 18:06:19,681 INFO] Epoch: 219 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:20,070 INFO] Epoch: 219 | ValAcc: 0.8960 | ValLoss: 0.0480 \n",
      "\n",
      "[2017-12-02 18:06:20,851 INFO] Epoch: 220 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:21,198 INFO] Epoch: 220 | ValAcc: 0.8970 | ValLoss: 0.0480 \n",
      "\n",
      "[2017-12-02 18:06:21,912 INFO] Epoch: 221 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:22,281 INFO] Epoch: 221 | ValAcc: 0.8990 | ValLoss: 0.0480 \n",
      "\n",
      "[2017-12-02 18:06:22,975 INFO] Epoch: 222 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:23,337 INFO] Epoch: 222 | ValAcc: 0.8990 | ValLoss: 0.0480 \n",
      "\n",
      "[2017-12-02 18:06:24,042 INFO] Epoch: 223 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:24,465 INFO] Epoch: 223 | ValAcc: 0.8990 | ValLoss: 0.0481 \n",
      "\n",
      "[2017-12-02 18:06:25,159 INFO] Epoch: 224 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:25,493 INFO] Epoch: 224 | ValAcc: 0.8970 | ValLoss: 0.0483 \n",
      "\n",
      "[2017-12-02 18:06:26,185 INFO] Epoch: 225 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:26,534 INFO] Epoch: 225 | ValAcc: 0.8970 | ValLoss: 0.0481 \n",
      "\n",
      "[2017-12-02 18:06:27,186 INFO] Epoch: 226 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:27,521 INFO] Epoch: 226 | ValAcc: 0.8970 | ValLoss: 0.0483 \n",
      "\n",
      "[2017-12-02 18:06:28,169 INFO] Epoch: 227 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:28,489 INFO] Epoch: 227 | ValAcc: 0.8980 | ValLoss: 0.0483 \n",
      "\n",
      "[2017-12-02 18:06:29,133 INFO] Epoch: 228 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:29,453 INFO] Epoch: 228 | ValAcc: 0.8970 | ValLoss: 0.0481 \n",
      "\n",
      "[2017-12-02 18:06:30,161 INFO] Epoch: 229 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:30,477 INFO] Epoch: 229 | ValAcc: 0.8970 | ValLoss: 0.0483 \n",
      "\n",
      "[2017-12-02 18:06:31,154 INFO] Epoch: 230 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:31,481 INFO] Epoch: 230 | ValAcc: 0.8960 | ValLoss: 0.0484 \n",
      "\n",
      "[2017-12-02 18:06:32,154 INFO] Epoch: 231 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:32,484 INFO] Epoch: 231 | ValAcc: 0.8960 | ValLoss: 0.0484 \n",
      "\n",
      "[2017-12-02 18:06:33,136 INFO] Epoch: 232 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:33,471 INFO] Epoch: 232 | ValAcc: 0.8960 | ValLoss: 0.0483 \n",
      "\n",
      "[2017-12-02 18:06:34,180 INFO] Epoch: 233 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:34,539 INFO] Epoch: 233 | ValAcc: 0.8970 | ValLoss: 0.0481 \n",
      "\n",
      "[2017-12-02 18:06:35,270 INFO] Epoch: 234 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:35,629 INFO] Epoch: 234 | ValAcc: 0.8970 | ValLoss: 0.0482 \n",
      "\n",
      "[2017-12-02 18:06:36,414 INFO] Epoch: 235 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:36,760 INFO] Epoch: 235 | ValAcc: 0.8970 | ValLoss: 0.0481 \n",
      "\n",
      "[2017-12-02 18:06:37,517 INFO] Epoch: 236 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:37,823 INFO] Epoch: 236 | ValAcc: 0.8970 | ValLoss: 0.0482 \n",
      "\n",
      "[2017-12-02 18:06:38,548 INFO] Epoch: 237 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:38,934 INFO] Epoch: 237 | ValAcc: 0.8970 | ValLoss: 0.0483 \n",
      "\n",
      "[2017-12-02 18:06:39,683 INFO] Epoch: 238 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:40,051 INFO] Epoch: 238 | ValAcc: 0.8960 | ValLoss: 0.0483 \n",
      "\n",
      "[2017-12-02 18:06:40,830 INFO] Epoch: 239 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:41,213 INFO] Epoch: 239 | ValAcc: 0.8970 | ValLoss: 0.0485 \n",
      "\n",
      "[2017-12-02 18:06:42,008 INFO] Epoch: 240 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:42,379 INFO] Epoch: 240 | ValAcc: 0.8970 | ValLoss: 0.0484 \n",
      "\n",
      "[2017-12-02 18:06:43,086 INFO] Epoch: 241 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:43,468 INFO] Epoch: 241 | ValAcc: 0.8950 | ValLoss: 0.0484 \n",
      "\n",
      "[2017-12-02 18:06:44,212 INFO] Epoch: 242 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:44,564 INFO] Epoch: 242 | ValAcc: 0.8960 | ValLoss: 0.0485 \n",
      "\n",
      "[2017-12-02 18:06:45,337 INFO] Epoch: 243 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:45,735 INFO] Epoch: 243 | ValAcc: 0.8950 | ValLoss: 0.0483 \n",
      "\n",
      "[2017-12-02 18:06:46,532 INFO] Epoch: 244 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:46,905 INFO] Epoch: 244 | ValAcc: 0.8970 | ValLoss: 0.0483 \n",
      "\n",
      "[2017-12-02 18:06:47,685 INFO] Epoch: 245 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:48,051 INFO] Epoch: 245 | ValAcc: 0.8960 | ValLoss: 0.0484 \n",
      "\n",
      "[2017-12-02 18:06:48,787 INFO] Epoch: 246 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:49,122 INFO] Epoch: 246 | ValAcc: 0.8960 | ValLoss: 0.0484 \n",
      "\n",
      "[2017-12-02 18:06:49,804 INFO] Epoch: 247 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:50,158 INFO] Epoch: 247 | ValAcc: 0.8980 | ValLoss: 0.0484 \n",
      "\n",
      "[2017-12-02 18:06:50,923 INFO] Epoch: 248 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:51,275 INFO] Epoch: 248 | ValAcc: 0.8980 | ValLoss: 0.0483 \n",
      "\n",
      "[2017-12-02 18:06:51,959 INFO] Epoch: 249 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:52,278 INFO] Epoch: 249 | ValAcc: 0.8990 | ValLoss: 0.0484 \n",
      "\n",
      "[2017-12-02 18:06:52,949 INFO] Epoch: 250 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:53,278 INFO] Epoch: 250 | ValAcc: 0.8970 | ValLoss: 0.0485 \n",
      "\n",
      "[2017-12-02 18:06:53,924 INFO] Epoch: 251 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:54,252 INFO] Epoch: 251 | ValAcc: 0.8960 | ValLoss: 0.0485 \n",
      "\n",
      "[2017-12-02 18:06:54,928 INFO] Epoch: 252 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:55,246 INFO] Epoch: 252 | ValAcc: 0.8970 | ValLoss: 0.0487 \n",
      "\n",
      "[2017-12-02 18:06:55,885 INFO] Epoch: 253 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:56,199 INFO] Epoch: 253 | ValAcc: 0.8970 | ValLoss: 0.0484 \n",
      "\n",
      "[2017-12-02 18:06:56,844 INFO] Epoch: 254 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:57,162 INFO] Epoch: 254 | ValAcc: 0.8960 | ValLoss: 0.0486 \n",
      "\n",
      "[2017-12-02 18:06:57,806 INFO] Epoch: 255 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:58,121 INFO] Epoch: 255 | ValAcc: 0.8970 | ValLoss: 0.0486 \n",
      "\n",
      "[2017-12-02 18:06:58,772 INFO] Epoch: 256 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:06:59,104 INFO] Epoch: 256 | ValAcc: 0.8950 | ValLoss: 0.0488 \n",
      "\n",
      "[2017-12-02 18:06:59,738 INFO] Epoch: 257 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:00,056 INFO] Epoch: 257 | ValAcc: 0.8970 | ValLoss: 0.0487 \n",
      "\n",
      "[2017-12-02 18:07:00,681 INFO] Epoch: 258 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:07:01,001 INFO] Epoch: 258 | ValAcc: 0.8960 | ValLoss: 0.0486 \n",
      "\n",
      "[2017-12-02 18:07:01,642 INFO] Epoch: 259 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:01,972 INFO] Epoch: 259 | ValAcc: 0.8990 | ValLoss: 0.0484 \n",
      "\n",
      "[2017-12-02 18:07:02,598 INFO] Epoch: 260 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:02,907 INFO] Epoch: 260 | ValAcc: 0.8980 | ValLoss: 0.0486 \n",
      "\n",
      "[2017-12-02 18:07:03,544 INFO] Epoch: 261 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:03,859 INFO] Epoch: 261 | ValAcc: 0.8960 | ValLoss: 0.0487 \n",
      "\n",
      "[2017-12-02 18:07:04,499 INFO] Epoch: 262 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:04,808 INFO] Epoch: 262 | ValAcc: 0.8950 | ValLoss: 0.0487 \n",
      "\n",
      "[2017-12-02 18:07:05,460 INFO] Epoch: 263 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:05,781 INFO] Epoch: 263 | ValAcc: 0.8980 | ValLoss: 0.0486 \n",
      "\n",
      "[2017-12-02 18:07:06,423 INFO] Epoch: 264 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:06,758 INFO] Epoch: 264 | ValAcc: 0.8990 | ValLoss: 0.0486 \n",
      "\n",
      "[2017-12-02 18:07:07,403 INFO] Epoch: 265 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:07,718 INFO] Epoch: 265 | ValAcc: 0.8970 | ValLoss: 0.0486 \n",
      "\n",
      "[2017-12-02 18:07:08,352 INFO] Epoch: 266 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:08,670 INFO] Epoch: 266 | ValAcc: 0.8950 | ValLoss: 0.0489 \n",
      "\n",
      "[2017-12-02 18:07:09,322 INFO] Epoch: 267 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:09,636 INFO] Epoch: 267 | ValAcc: 0.8960 | ValLoss: 0.0489 \n",
      "\n",
      "[2017-12-02 18:07:10,274 INFO] Epoch: 268 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:10,583 INFO] Epoch: 268 | ValAcc: 0.8970 | ValLoss: 0.0488 \n",
      "\n",
      "[2017-12-02 18:07:11,207 INFO] Epoch: 269 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:11,532 INFO] Epoch: 269 | ValAcc: 0.8990 | ValLoss: 0.0487 \n",
      "\n",
      "[2017-12-02 18:07:12,211 INFO] Epoch: 270 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:12,527 INFO] Epoch: 270 | ValAcc: 0.8980 | ValLoss: 0.0487 \n",
      "\n",
      "[2017-12-02 18:07:13,169 INFO] Epoch: 271 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:13,473 INFO] Epoch: 271 | ValAcc: 0.8970 | ValLoss: 0.0487 \n",
      "\n",
      "[2017-12-02 18:07:14,105 INFO] Epoch: 272 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:14,430 INFO] Epoch: 272 | ValAcc: 0.8950 | ValLoss: 0.0487 \n",
      "\n",
      "[2017-12-02 18:07:15,066 INFO] Epoch: 273 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:15,385 INFO] Epoch: 273 | ValAcc: 0.8970 | ValLoss: 0.0488 \n",
      "\n",
      "[2017-12-02 18:07:16,004 INFO] Epoch: 274 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:16,322 INFO] Epoch: 274 | ValAcc: 0.8960 | ValLoss: 0.0488 \n",
      "\n",
      "[2017-12-02 18:07:16,965 INFO] Epoch: 275 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:17,276 INFO] Epoch: 275 | ValAcc: 0.8970 | ValLoss: 0.0489 \n",
      "\n",
      "[2017-12-02 18:07:17,924 INFO] Epoch: 276 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:18,239 INFO] Epoch: 276 | ValAcc: 0.8970 | ValLoss: 0.0488 \n",
      "\n",
      "[2017-12-02 18:07:18,869 INFO] Epoch: 277 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:19,181 INFO] Epoch: 277 | ValAcc: 0.8960 | ValLoss: 0.0488 \n",
      "\n",
      "[2017-12-02 18:07:19,821 INFO] Epoch: 278 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:20,130 INFO] Epoch: 278 | ValAcc: 0.8970 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:20,771 INFO] Epoch: 279 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:21,088 INFO] Epoch: 279 | ValAcc: 0.8970 | ValLoss: 0.0489 \n",
      "\n",
      "[2017-12-02 18:07:21,719 INFO] Epoch: 280 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:22,033 INFO] Epoch: 280 | ValAcc: 0.8970 | ValLoss: 0.0489 \n",
      "\n",
      "[2017-12-02 18:07:22,666 INFO] Epoch: 281 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:22,992 INFO] Epoch: 281 | ValAcc: 0.8960 | ValLoss: 0.0490 \n",
      "\n",
      "[2017-12-02 18:07:23,626 INFO] Epoch: 282 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:23,947 INFO] Epoch: 282 | ValAcc: 0.8970 | ValLoss: 0.0490 \n",
      "\n",
      "[2017-12-02 18:07:24,569 INFO] Epoch: 283 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:24,888 INFO] Epoch: 283 | ValAcc: 0.8970 | ValLoss: 0.0490 \n",
      "\n",
      "[2017-12-02 18:07:25,535 INFO] Epoch: 284 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:25,845 INFO] Epoch: 284 | ValAcc: 0.8990 | ValLoss: 0.0489 \n",
      "\n",
      "[2017-12-02 18:07:26,480 INFO] Epoch: 285 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:26,790 INFO] Epoch: 285 | ValAcc: 0.8990 | ValLoss: 0.0489 \n",
      "\n",
      "[2017-12-02 18:07:27,446 INFO] Epoch: 286 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:27,754 INFO] Epoch: 286 | ValAcc: 0.8970 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:28,389 INFO] Epoch: 287 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:28,708 INFO] Epoch: 287 | ValAcc: 0.8960 | ValLoss: 0.0492 \n",
      "\n",
      "[2017-12-02 18:07:29,343 INFO] Epoch: 288 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:29,660 INFO] Epoch: 288 | ValAcc: 0.8970 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:30,279 INFO] Epoch: 289 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:30,598 INFO] Epoch: 289 | ValAcc: 0.8960 | ValLoss: 0.0490 \n",
      "\n",
      "[2017-12-02 18:07:31,233 INFO] Epoch: 290 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:31,554 INFO] Epoch: 290 | ValAcc: 0.8960 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:32,186 INFO] Epoch: 291 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:32,516 INFO] Epoch: 291 | ValAcc: 0.8970 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:33,152 INFO] Epoch: 292 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:33,470 INFO] Epoch: 292 | ValAcc: 0.8970 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:34,091 INFO] Epoch: 293 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:34,400 INFO] Epoch: 293 | ValAcc: 0.8960 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:35,029 INFO] Epoch: 294 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:35,345 INFO] Epoch: 294 | ValAcc: 0.8970 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:35,978 INFO] Epoch: 295 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:36,301 INFO] Epoch: 295 | ValAcc: 0.8980 | ValLoss: 0.0489 \n",
      "\n",
      "[2017-12-02 18:07:36,942 INFO] Epoch: 296 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:37,258 INFO] Epoch: 296 | ValAcc: 0.8980 | ValLoss: 0.0490 \n",
      "\n",
      "[2017-12-02 18:07:37,924 INFO] Epoch: 297 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:38,240 INFO] Epoch: 297 | ValAcc: 0.8980 | ValLoss: 0.0490 \n",
      "\n",
      "[2017-12-02 18:07:38,855 INFO] Epoch: 298 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:39,165 INFO] Epoch: 298 | ValAcc: 0.8980 | ValLoss: 0.0490 \n",
      "\n",
      "[2017-12-02 18:07:39,800 INFO] Epoch: 299 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:40,113 INFO] Epoch: 299 | ValAcc: 0.8980 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:40,751 INFO] Epoch: 300 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:41,060 INFO] Epoch: 300 | ValAcc: 0.8970 | ValLoss: 0.0492 \n",
      "\n",
      "[2017-12-02 18:07:41,700 INFO] Epoch: 301 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:42,017 INFO] Epoch: 301 | ValAcc: 0.8950 | ValLoss: 0.0492 \n",
      "\n",
      "[2017-12-02 18:07:42,666 INFO] Epoch: 302 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:42,987 INFO] Epoch: 302 | ValAcc: 0.8970 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:43,624 INFO] Epoch: 303 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:43,944 INFO] Epoch: 303 | ValAcc: 0.8960 | ValLoss: 0.0492 \n",
      "\n",
      "[2017-12-02 18:07:44,581 INFO] Epoch: 304 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:44,889 INFO] Epoch: 304 | ValAcc: 0.8990 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:45,536 INFO] Epoch: 305 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:45,848 INFO] Epoch: 305 | ValAcc: 0.8980 | ValLoss: 0.0492 \n",
      "\n",
      "[2017-12-02 18:07:46,477 INFO] Epoch: 306 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:46,803 INFO] Epoch: 306 | ValAcc: 0.8980 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:47,465 INFO] Epoch: 307 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:47,776 INFO] Epoch: 307 | ValAcc: 0.8970 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:48,413 INFO] Epoch: 308 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:48,723 INFO] Epoch: 308 | ValAcc: 0.8960 | ValLoss: 0.0493 \n",
      "\n",
      "[2017-12-02 18:07:49,354 INFO] Epoch: 309 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:49,659 INFO] Epoch: 309 | ValAcc: 0.8950 | ValLoss: 0.0494 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:07:50,287 INFO] Epoch: 310 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:50,596 INFO] Epoch: 310 | ValAcc: 0.8980 | ValLoss: 0.0492 \n",
      "\n",
      "[2017-12-02 18:07:51,243 INFO] Epoch: 311 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:51,554 INFO] Epoch: 311 | ValAcc: 0.8980 | ValLoss: 0.0491 \n",
      "\n",
      "[2017-12-02 18:07:52,184 INFO] Epoch: 312 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:52,516 INFO] Epoch: 312 | ValAcc: 0.8960 | ValLoss: 0.0494 \n",
      "\n",
      "[2017-12-02 18:07:53,143 INFO] Epoch: 313 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:53,484 INFO] Epoch: 313 | ValAcc: 0.8960 | ValLoss: 0.0495 \n",
      "\n",
      "[2017-12-02 18:07:54,107 INFO] Epoch: 314 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:54,413 INFO] Epoch: 314 | ValAcc: 0.8970 | ValLoss: 0.0494 \n",
      "\n",
      "[2017-12-02 18:07:55,065 INFO] Epoch: 315 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:55,410 INFO] Epoch: 315 | ValAcc: 0.8980 | ValLoss: 0.0493 \n",
      "\n",
      "[2017-12-02 18:07:56,039 INFO] Epoch: 316 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:56,353 INFO] Epoch: 316 | ValAcc: 0.8980 | ValLoss: 0.0493 \n",
      "\n",
      "[2017-12-02 18:07:56,974 INFO] Epoch: 317 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:57,287 INFO] Epoch: 317 | ValAcc: 0.8970 | ValLoss: 0.0493 \n",
      "\n",
      "[2017-12-02 18:07:57,927 INFO] Epoch: 318 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:58,239 INFO] Epoch: 318 | ValAcc: 0.8980 | ValLoss: 0.0492 \n",
      "\n",
      "[2017-12-02 18:07:58,869 INFO] Epoch: 319 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:07:59,186 INFO] Epoch: 319 | ValAcc: 0.8980 | ValLoss: 0.0493 \n",
      "\n",
      "[2017-12-02 18:07:59,821 INFO] Epoch: 320 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:00,138 INFO] Epoch: 320 | ValAcc: 0.8960 | ValLoss: 0.0495 \n",
      "\n",
      "[2017-12-02 18:08:00,770 INFO] Epoch: 321 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:01,082 INFO] Epoch: 321 | ValAcc: 0.8980 | ValLoss: 0.0494 \n",
      "\n",
      "[2017-12-02 18:08:01,723 INFO] Epoch: 322 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:02,037 INFO] Epoch: 322 | ValAcc: 0.8960 | ValLoss: 0.0495 \n",
      "\n",
      "[2017-12-02 18:08:02,685 INFO] Epoch: 323 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:03,007 INFO] Epoch: 323 | ValAcc: 0.8980 | ValLoss: 0.0494 \n",
      "\n",
      "[2017-12-02 18:08:03,633 INFO] Epoch: 324 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:03,947 INFO] Epoch: 324 | ValAcc: 0.8980 | ValLoss: 0.0493 \n",
      "\n",
      "[2017-12-02 18:08:04,566 INFO] Epoch: 325 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:04,883 INFO] Epoch: 325 | ValAcc: 0.8980 | ValLoss: 0.0494 \n",
      "\n",
      "[2017-12-02 18:08:05,537 INFO] Epoch: 326 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:05,858 INFO] Epoch: 326 | ValAcc: 0.8980 | ValLoss: 0.0494 \n",
      "\n",
      "[2017-12-02 18:08:06,515 INFO] Epoch: 327 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:06,838 INFO] Epoch: 327 | ValAcc: 0.8970 | ValLoss: 0.0494 \n",
      "\n",
      "[2017-12-02 18:08:07,484 INFO] Epoch: 328 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:07,808 INFO] Epoch: 328 | ValAcc: 0.8980 | ValLoss: 0.0494 \n",
      "\n",
      "[2017-12-02 18:08:08,462 INFO] Epoch: 329 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:08,782 INFO] Epoch: 329 | ValAcc: 0.8970 | ValLoss: 0.0494 \n",
      "\n",
      "[2017-12-02 18:08:09,421 INFO] Epoch: 330 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:09,740 INFO] Epoch: 330 | ValAcc: 0.8980 | ValLoss: 0.0495 \n",
      "\n",
      "[2017-12-02 18:08:10,390 INFO] Epoch: 331 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:10,716 INFO] Epoch: 331 | ValAcc: 0.8980 | ValLoss: 0.0495 \n",
      "\n",
      "[2017-12-02 18:08:11,348 INFO] Epoch: 332 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:11,681 INFO] Epoch: 332 | ValAcc: 0.8970 | ValLoss: 0.0495 \n",
      "\n",
      "[2017-12-02 18:08:12,307 INFO] Epoch: 333 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:12,626 INFO] Epoch: 333 | ValAcc: 0.8970 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:13,265 INFO] Epoch: 334 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:13,570 INFO] Epoch: 334 | ValAcc: 0.8980 | ValLoss: 0.0495 \n",
      "\n",
      "[2017-12-02 18:08:14,206 INFO] Epoch: 335 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:14,533 INFO] Epoch: 335 | ValAcc: 0.8960 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:15,201 INFO] Epoch: 336 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:15,598 INFO] Epoch: 336 | ValAcc: 0.8980 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:16,341 INFO] Epoch: 337 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:16,666 INFO] Epoch: 337 | ValAcc: 0.8960 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:17,346 INFO] Epoch: 338 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:17,737 INFO] Epoch: 338 | ValAcc: 0.8970 | ValLoss: 0.0495 \n",
      "\n",
      "[2017-12-02 18:08:18,481 INFO] Epoch: 339 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:18,832 INFO] Epoch: 339 | ValAcc: 0.8970 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:19,545 INFO] Epoch: 340 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:19,870 INFO] Epoch: 340 | ValAcc: 0.8980 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:20,506 INFO] Epoch: 341 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:20,823 INFO] Epoch: 341 | ValAcc: 0.8960 | ValLoss: 0.0497 \n",
      "\n",
      "[2017-12-02 18:08:21,464 INFO] Epoch: 342 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:21,785 INFO] Epoch: 342 | ValAcc: 0.8970 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:22,427 INFO] Epoch: 343 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:22,752 INFO] Epoch: 343 | ValAcc: 0.8970 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:23,389 INFO] Epoch: 344 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:23,725 INFO] Epoch: 344 | ValAcc: 0.8980 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:24,347 INFO] Epoch: 345 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:24,658 INFO] Epoch: 345 | ValAcc: 0.8970 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:25,300 INFO] Epoch: 346 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:25,616 INFO] Epoch: 346 | ValAcc: 0.8970 | ValLoss: 0.0495 \n",
      "\n",
      "[2017-12-02 18:08:26,254 INFO] Epoch: 347 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:26,577 INFO] Epoch: 347 | ValAcc: 0.8970 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:27,233 INFO] Epoch: 348 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:27,550 INFO] Epoch: 348 | ValAcc: 0.8970 | ValLoss: 0.0497 \n",
      "\n",
      "[2017-12-02 18:08:28,196 INFO] Epoch: 349 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:28,505 INFO] Epoch: 349 | ValAcc: 0.8970 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:29,141 INFO] Epoch: 350 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:29,460 INFO] Epoch: 350 | ValAcc: 0.8960 | ValLoss: 0.0497 \n",
      "\n",
      "[2017-12-02 18:08:30,104 INFO] Epoch: 351 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:30,418 INFO] Epoch: 351 | ValAcc: 0.8960 | ValLoss: 0.0496 \n",
      "\n",
      "[2017-12-02 18:08:31,053 INFO] Epoch: 352 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:31,377 INFO] Epoch: 352 | ValAcc: 0.8970 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:32,021 INFO] Epoch: 353 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:32,330 INFO] Epoch: 353 | ValAcc: 0.8970 | ValLoss: 0.0497 \n",
      "\n",
      "[2017-12-02 18:08:32,975 INFO] Epoch: 354 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:33,290 INFO] Epoch: 354 | ValAcc: 0.8970 | ValLoss: 0.0497 \n",
      "\n",
      "[2017-12-02 18:08:33,908 INFO] Epoch: 355 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:34,245 INFO] Epoch: 355 | ValAcc: 0.8970 | ValLoss: 0.0499 \n",
      "\n",
      "[2017-12-02 18:08:34,905 INFO] Epoch: 356 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:35,220 INFO] Epoch: 356 | ValAcc: 0.8980 | ValLoss: 0.0497 \n",
      "\n",
      "[2017-12-02 18:08:35,860 INFO] Epoch: 357 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:36,180 INFO] Epoch: 357 | ValAcc: 0.8980 | ValLoss: 0.0497 \n",
      "\n",
      "[2017-12-02 18:08:36,823 INFO] Epoch: 358 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:37,138 INFO] Epoch: 358 | ValAcc: 0.8980 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:37,778 INFO] Epoch: 359 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:38,095 INFO] Epoch: 359 | ValAcc: 0.8970 | ValLoss: 0.0499 \n",
      "\n",
      "[2017-12-02 18:08:38,734 INFO] Epoch: 360 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:39,134 INFO] Epoch: 360 | ValAcc: 0.8980 | ValLoss: 0.0499 \n",
      "\n",
      "[2017-12-02 18:08:39,861 INFO] Epoch: 361 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:08:40,206 INFO] Epoch: 361 | ValAcc: 0.8970 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:40,903 INFO] Epoch: 362 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:41,298 INFO] Epoch: 362 | ValAcc: 0.8970 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:41,976 INFO] Epoch: 363 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:42,338 INFO] Epoch: 363 | ValAcc: 0.8960 | ValLoss: 0.0499 \n",
      "\n",
      "[2017-12-02 18:08:43,099 INFO] Epoch: 364 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:43,431 INFO] Epoch: 364 | ValAcc: 0.8980 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:44,097 INFO] Epoch: 365 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:44,463 INFO] Epoch: 365 | ValAcc: 0.8970 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:45,133 INFO] Epoch: 366 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:45,494 INFO] Epoch: 366 | ValAcc: 0.8950 | ValLoss: 0.0501 \n",
      "\n",
      "[2017-12-02 18:08:46,224 INFO] Epoch: 367 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:46,574 INFO] Epoch: 367 | ValAcc: 0.8960 | ValLoss: 0.0500 \n",
      "\n",
      "[2017-12-02 18:08:47,394 INFO] Epoch: 368 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:47,756 INFO] Epoch: 368 | ValAcc: 0.8960 | ValLoss: 0.0499 \n",
      "\n",
      "[2017-12-02 18:08:48,486 INFO] Epoch: 369 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:48,820 INFO] Epoch: 369 | ValAcc: 0.8970 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:49,505 INFO] Epoch: 370 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:49,846 INFO] Epoch: 370 | ValAcc: 0.8970 | ValLoss: 0.0500 \n",
      "\n",
      "[2017-12-02 18:08:50,579 INFO] Epoch: 371 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:50,908 INFO] Epoch: 371 | ValAcc: 0.8970 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:51,588 INFO] Epoch: 372 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:51,934 INFO] Epoch: 372 | ValAcc: 0.8970 | ValLoss: 0.0498 \n",
      "\n",
      "[2017-12-02 18:08:52,596 INFO] Epoch: 373 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:52,906 INFO] Epoch: 373 | ValAcc: 0.8970 | ValLoss: 0.0499 \n",
      "\n",
      "[2017-12-02 18:08:53,616 INFO] Epoch: 374 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:53,963 INFO] Epoch: 374 | ValAcc: 0.8970 | ValLoss: 0.0500 \n",
      "\n",
      "[2017-12-02 18:08:54,649 INFO] Epoch: 375 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:54,994 INFO] Epoch: 375 | ValAcc: 0.8970 | ValLoss: 0.0501 \n",
      "\n",
      "[2017-12-02 18:08:55,657 INFO] Epoch: 376 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:56,013 INFO] Epoch: 376 | ValAcc: 0.8970 | ValLoss: 0.0499 \n",
      "\n",
      "[2017-12-02 18:08:56,680 INFO] Epoch: 377 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:57,013 INFO] Epoch: 377 | ValAcc: 0.8970 | ValLoss: 0.0500 \n",
      "\n",
      "[2017-12-02 18:08:57,660 INFO] Epoch: 378 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:58,005 INFO] Epoch: 378 | ValAcc: 0.8970 | ValLoss: 0.0501 \n",
      "\n",
      "[2017-12-02 18:08:58,695 INFO] Epoch: 379 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:08:59,026 INFO] Epoch: 379 | ValAcc: 0.8970 | ValLoss: 0.0500 \n",
      "\n",
      "[2017-12-02 18:08:59,695 INFO] Epoch: 380 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:00,005 INFO] Epoch: 380 | ValAcc: 0.8960 | ValLoss: 0.0500 \n",
      "\n",
      "[2017-12-02 18:09:00,667 INFO] Epoch: 381 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:01,001 INFO] Epoch: 381 | ValAcc: 0.8970 | ValLoss: 0.0500 \n",
      "\n",
      "[2017-12-02 18:09:01,661 INFO] Epoch: 382 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:01,976 INFO] Epoch: 382 | ValAcc: 0.8970 | ValLoss: 0.0500 \n",
      "\n",
      "[2017-12-02 18:09:02,653 INFO] Epoch: 383 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:03,005 INFO] Epoch: 383 | ValAcc: 0.8970 | ValLoss: 0.0501 \n",
      "\n",
      "[2017-12-02 18:09:03,713 INFO] Epoch: 384 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:04,027 INFO] Epoch: 384 | ValAcc: 0.8960 | ValLoss: 0.0502 \n",
      "\n",
      "[2017-12-02 18:09:04,679 INFO] Epoch: 385 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:04,996 INFO] Epoch: 385 | ValAcc: 0.8960 | ValLoss: 0.0502 \n",
      "\n",
      "[2017-12-02 18:09:05,642 INFO] Epoch: 386 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:05,960 INFO] Epoch: 386 | ValAcc: 0.8950 | ValLoss: 0.0502 \n",
      "\n",
      "[2017-12-02 18:09:06,630 INFO] Epoch: 387 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:06,989 INFO] Epoch: 387 | ValAcc: 0.8970 | ValLoss: 0.0501 \n",
      "\n",
      "[2017-12-02 18:09:07,698 INFO] Epoch: 388 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:08,039 INFO] Epoch: 388 | ValAcc: 0.8970 | ValLoss: 0.0501 \n",
      "\n",
      "[2017-12-02 18:09:08,731 INFO] Epoch: 389 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:09,131 INFO] Epoch: 389 | ValAcc: 0.8960 | ValLoss: 0.0502 \n",
      "\n",
      "[2017-12-02 18:09:09,798 INFO] Epoch: 390 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:10,128 INFO] Epoch: 390 | ValAcc: 0.8970 | ValLoss: 0.0501 \n",
      "\n",
      "[2017-12-02 18:09:10,776 INFO] Epoch: 391 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:11,106 INFO] Epoch: 391 | ValAcc: 0.8970 | ValLoss: 0.0501 \n",
      "\n",
      "[2017-12-02 18:09:11,787 INFO] Epoch: 392 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:12,115 INFO] Epoch: 392 | ValAcc: 0.8960 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:12,782 INFO] Epoch: 393 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:13,101 INFO] Epoch: 393 | ValAcc: 0.8960 | ValLoss: 0.0501 \n",
      "\n",
      "[2017-12-02 18:09:13,751 INFO] Epoch: 394 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:14,059 INFO] Epoch: 394 | ValAcc: 0.8950 | ValLoss: 0.0501 \n",
      "\n",
      "[2017-12-02 18:09:14,679 INFO] Epoch: 395 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:14,998 INFO] Epoch: 395 | ValAcc: 0.8960 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:15,647 INFO] Epoch: 396 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:15,967 INFO] Epoch: 396 | ValAcc: 0.8970 | ValLoss: 0.0502 \n",
      "\n",
      "[2017-12-02 18:09:16,589 INFO] Epoch: 397 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:16,898 INFO] Epoch: 397 | ValAcc: 0.8970 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:17,534 INFO] Epoch: 398 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:17,846 INFO] Epoch: 398 | ValAcc: 0.8950 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:18,481 INFO] Epoch: 399 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:18,803 INFO] Epoch: 399 | ValAcc: 0.8960 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:19,428 INFO] Epoch: 400 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:19,740 INFO] Epoch: 400 | ValAcc: 0.8960 | ValLoss: 0.0502 \n",
      "\n",
      "[2017-12-02 18:09:20,394 INFO] Epoch: 401 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:20,700 INFO] Epoch: 401 | ValAcc: 0.8970 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:21,322 INFO] Epoch: 402 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:21,628 INFO] Epoch: 402 | ValAcc: 0.8970 | ValLoss: 0.0502 \n",
      "\n",
      "[2017-12-02 18:09:22,264 INFO] Epoch: 403 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:22,574 INFO] Epoch: 403 | ValAcc: 0.8960 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:23,176 INFO] Epoch: 404 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:23,492 INFO] Epoch: 404 | ValAcc: 0.8950 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:24,127 INFO] Epoch: 405 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:24,437 INFO] Epoch: 405 | ValAcc: 0.8960 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:25,063 INFO] Epoch: 406 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:25,376 INFO] Epoch: 406 | ValAcc: 0.8970 | ValLoss: 0.0502 \n",
      "\n",
      "[2017-12-02 18:09:25,992 INFO] Epoch: 407 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:26,300 INFO] Epoch: 407 | ValAcc: 0.8970 | ValLoss: 0.0504 \n",
      "\n",
      "[2017-12-02 18:09:26,952 INFO] Epoch: 408 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:27,258 INFO] Epoch: 408 | ValAcc: 0.8970 | ValLoss: 0.0504 \n",
      "\n",
      "[2017-12-02 18:09:27,874 INFO] Epoch: 409 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:28,197 INFO] Epoch: 409 | ValAcc: 0.8960 | ValLoss: 0.0504 \n",
      "\n",
      "[2017-12-02 18:09:28,809 INFO] Epoch: 410 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:29,112 INFO] Epoch: 410 | ValAcc: 0.8970 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:29,743 INFO] Epoch: 411 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:30,052 INFO] Epoch: 411 | ValAcc: 0.8960 | ValLoss: 0.0505 \n",
      "\n",
      "[2017-12-02 18:09:30,673 INFO] Epoch: 412 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:30,982 INFO] Epoch: 412 | ValAcc: 0.8970 | ValLoss: 0.0503 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:09:31,607 INFO] Epoch: 413 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:31,918 INFO] Epoch: 413 | ValAcc: 0.8970 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:32,545 INFO] Epoch: 414 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:32,850 INFO] Epoch: 414 | ValAcc: 0.8960 | ValLoss: 0.0504 \n",
      "\n",
      "[2017-12-02 18:09:33,492 INFO] Epoch: 415 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:33,803 INFO] Epoch: 415 | ValAcc: 0.8960 | ValLoss: 0.0504 \n",
      "\n",
      "[2017-12-02 18:09:34,430 INFO] Epoch: 416 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:34,729 INFO] Epoch: 416 | ValAcc: 0.8960 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:35,358 INFO] Epoch: 417 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:35,658 INFO] Epoch: 417 | ValAcc: 0.8970 | ValLoss: 0.0504 \n",
      "\n",
      "[2017-12-02 18:09:36,266 INFO] Epoch: 418 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:36,571 INFO] Epoch: 418 | ValAcc: 0.8970 | ValLoss: 0.0504 \n",
      "\n",
      "[2017-12-02 18:09:37,200 INFO] Epoch: 419 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:37,514 INFO] Epoch: 419 | ValAcc: 0.8950 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:38,135 INFO] Epoch: 420 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:38,441 INFO] Epoch: 420 | ValAcc: 0.8960 | ValLoss: 0.0505 \n",
      "\n",
      "[2017-12-02 18:09:39,078 INFO] Epoch: 421 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:39,383 INFO] Epoch: 421 | ValAcc: 0.8960 | ValLoss: 0.0503 \n",
      "\n",
      "[2017-12-02 18:09:40,013 INFO] Epoch: 422 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:40,329 INFO] Epoch: 422 | ValAcc: 0.8970 | ValLoss: 0.0504 \n",
      "\n",
      "[2017-12-02 18:09:40,956 INFO] Epoch: 423 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:41,261 INFO] Epoch: 423 | ValAcc: 0.8960 | ValLoss: 0.0504 \n",
      "\n",
      "[2017-12-02 18:09:41,905 INFO] Epoch: 424 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:42,215 INFO] Epoch: 424 | ValAcc: 0.8960 | ValLoss: 0.0505 \n",
      "\n",
      "[2017-12-02 18:09:42,830 INFO] Epoch: 425 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:43,136 INFO] Epoch: 425 | ValAcc: 0.8970 | ValLoss: 0.0505 \n",
      "\n",
      "[2017-12-02 18:09:43,781 INFO] Epoch: 426 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:44,101 INFO] Epoch: 426 | ValAcc: 0.8960 | ValLoss: 0.0505 \n",
      "\n",
      "[2017-12-02 18:09:44,712 INFO] Epoch: 427 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:45,030 INFO] Epoch: 427 | ValAcc: 0.8950 | ValLoss: 0.0506 \n",
      "\n",
      "[2017-12-02 18:09:45,671 INFO] Epoch: 428 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:45,977 INFO] Epoch: 428 | ValAcc: 0.8960 | ValLoss: 0.0505 \n",
      "\n",
      "[2017-12-02 18:09:46,608 INFO] Epoch: 429 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:46,914 INFO] Epoch: 429 | ValAcc: 0.8960 | ValLoss: 0.0506 \n",
      "\n",
      "[2017-12-02 18:09:47,545 INFO] Epoch: 430 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:47,851 INFO] Epoch: 430 | ValAcc: 0.8950 | ValLoss: 0.0506 \n",
      "\n",
      "[2017-12-02 18:09:48,488 INFO] Epoch: 431 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:48,786 INFO] Epoch: 431 | ValAcc: 0.8960 | ValLoss: 0.0506 \n",
      "\n",
      "[2017-12-02 18:09:49,397 INFO] Epoch: 432 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:49,704 INFO] Epoch: 432 | ValAcc: 0.8950 | ValLoss: 0.0506 \n",
      "\n",
      "[2017-12-02 18:09:50,330 INFO] Epoch: 433 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:50,647 INFO] Epoch: 433 | ValAcc: 0.8960 | ValLoss: 0.0505 \n",
      "\n",
      "[2017-12-02 18:09:51,321 INFO] Epoch: 434 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:51,674 INFO] Epoch: 434 | ValAcc: 0.8950 | ValLoss: 0.0506 \n",
      "\n",
      "[2017-12-02 18:09:52,360 INFO] Epoch: 435 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:52,721 INFO] Epoch: 435 | ValAcc: 0.8960 | ValLoss: 0.0507 \n",
      "\n",
      "[2017-12-02 18:09:53,424 INFO] Epoch: 436 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:53,744 INFO] Epoch: 436 | ValAcc: 0.8980 | ValLoss: 0.0507 \n",
      "\n",
      "[2017-12-02 18:09:54,388 INFO] Epoch: 437 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:54,691 INFO] Epoch: 437 | ValAcc: 0.8950 | ValLoss: 0.0507 \n",
      "\n",
      "[2017-12-02 18:09:55,346 INFO] Epoch: 438 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:55,656 INFO] Epoch: 438 | ValAcc: 0.8960 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:09:56,285 INFO] Epoch: 439 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:56,589 INFO] Epoch: 439 | ValAcc: 0.8960 | ValLoss: 0.0507 \n",
      "\n",
      "[2017-12-02 18:09:57,215 INFO] Epoch: 440 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:57,537 INFO] Epoch: 440 | ValAcc: 0.8970 | ValLoss: 0.0507 \n",
      "\n",
      "[2017-12-02 18:09:58,152 INFO] Epoch: 441 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:58,465 INFO] Epoch: 441 | ValAcc: 0.8980 | ValLoss: 0.0507 \n",
      "\n",
      "[2017-12-02 18:09:59,089 INFO] Epoch: 442 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:09:59,400 INFO] Epoch: 442 | ValAcc: 0.8960 | ValLoss: 0.0507 \n",
      "\n",
      "[2017-12-02 18:10:00,045 INFO] Epoch: 443 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:00,361 INFO] Epoch: 443 | ValAcc: 0.8970 | ValLoss: 0.0506 \n",
      "\n",
      "[2017-12-02 18:10:00,980 INFO] Epoch: 444 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:01,286 INFO] Epoch: 444 | ValAcc: 0.8960 | ValLoss: 0.0507 \n",
      "\n",
      "[2017-12-02 18:10:01,924 INFO] Epoch: 445 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:02,240 INFO] Epoch: 445 | ValAcc: 0.8960 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:02,869 INFO] Epoch: 446 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:03,175 INFO] Epoch: 446 | ValAcc: 0.8970 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:03,804 INFO] Epoch: 447 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:04,106 INFO] Epoch: 447 | ValAcc: 0.8970 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:04,734 INFO] Epoch: 448 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:05,046 INFO] Epoch: 448 | ValAcc: 0.8950 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:05,679 INFO] Epoch: 449 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:05,988 INFO] Epoch: 449 | ValAcc: 0.8980 | ValLoss: 0.0510 \n",
      "\n",
      "[2017-12-02 18:10:06,613 INFO] Epoch: 450 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:06,934 INFO] Epoch: 450 | ValAcc: 0.8950 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:07,564 INFO] Epoch: 451 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:07,870 INFO] Epoch: 451 | ValAcc: 0.8960 | ValLoss: 0.0507 \n",
      "\n",
      "[2017-12-02 18:10:08,483 INFO] Epoch: 452 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:08,793 INFO] Epoch: 452 | ValAcc: 0.8950 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:09,427 INFO] Epoch: 453 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:09,735 INFO] Epoch: 453 | ValAcc: 0.8950 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:10,351 INFO] Epoch: 454 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:10,659 INFO] Epoch: 454 | ValAcc: 0.8950 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:11,274 INFO] Epoch: 455 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:11,600 INFO] Epoch: 455 | ValAcc: 0.8950 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:12,233 INFO] Epoch: 456 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:12,541 INFO] Epoch: 456 | ValAcc: 0.8950 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:13,177 INFO] Epoch: 457 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:13,490 INFO] Epoch: 457 | ValAcc: 0.8950 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:14,139 INFO] Epoch: 458 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:14,468 INFO] Epoch: 458 | ValAcc: 0.8950 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:15,077 INFO] Epoch: 459 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:15,383 INFO] Epoch: 459 | ValAcc: 0.8950 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:15,998 INFO] Epoch: 460 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:16,307 INFO] Epoch: 460 | ValAcc: 0.8970 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:16,946 INFO] Epoch: 461 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:17,262 INFO] Epoch: 461 | ValAcc: 0.8970 | ValLoss: 0.0510 \n",
      "\n",
      "[2017-12-02 18:10:17,893 INFO] Epoch: 462 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:18,197 INFO] Epoch: 462 | ValAcc: 0.8970 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:18,806 INFO] Epoch: 463 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:19,109 INFO] Epoch: 463 | ValAcc: 0.8970 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:19,732 INFO] Epoch: 464 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:10:20,041 INFO] Epoch: 464 | ValAcc: 0.8970 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:20,686 INFO] Epoch: 465 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:21,002 INFO] Epoch: 465 | ValAcc: 0.8960 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:21,628 INFO] Epoch: 466 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:21,938 INFO] Epoch: 466 | ValAcc: 0.8960 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:22,567 INFO] Epoch: 467 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:22,871 INFO] Epoch: 467 | ValAcc: 0.8950 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:23,500 INFO] Epoch: 468 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:23,805 INFO] Epoch: 468 | ValAcc: 0.8960 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:24,428 INFO] Epoch: 469 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:24,736 INFO] Epoch: 469 | ValAcc: 0.8950 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:25,402 INFO] Epoch: 470 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:25,714 INFO] Epoch: 470 | ValAcc: 0.8960 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:26,345 INFO] Epoch: 471 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:26,657 INFO] Epoch: 471 | ValAcc: 0.8950 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:27,336 INFO] Epoch: 472 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:27,648 INFO] Epoch: 472 | ValAcc: 0.8950 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:28,288 INFO] Epoch: 473 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:28,593 INFO] Epoch: 473 | ValAcc: 0.8950 | ValLoss: 0.0510 \n",
      "\n",
      "[2017-12-02 18:10:29,203 INFO] Epoch: 474 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:29,523 INFO] Epoch: 474 | ValAcc: 0.8940 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:30,130 INFO] Epoch: 475 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:30,440 INFO] Epoch: 475 | ValAcc: 0.8950 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:31,060 INFO] Epoch: 476 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:31,377 INFO] Epoch: 476 | ValAcc: 0.8980 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:32,021 INFO] Epoch: 477 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:32,330 INFO] Epoch: 477 | ValAcc: 0.8960 | ValLoss: 0.0510 \n",
      "\n",
      "[2017-12-02 18:10:32,947 INFO] Epoch: 478 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:33,252 INFO] Epoch: 478 | ValAcc: 0.8950 | ValLoss: 0.0509 \n",
      "\n",
      "[2017-12-02 18:10:33,865 INFO] Epoch: 479 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:34,202 INFO] Epoch: 479 | ValAcc: 0.8960 | ValLoss: 0.0508 \n",
      "\n",
      "[2017-12-02 18:10:34,817 INFO] Epoch: 480 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:35,131 INFO] Epoch: 480 | ValAcc: 0.8960 | ValLoss: 0.0510 \n",
      "\n",
      "[2017-12-02 18:10:35,747 INFO] Epoch: 481 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:36,065 INFO] Epoch: 481 | ValAcc: 0.8950 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:36,677 INFO] Epoch: 482 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:36,976 INFO] Epoch: 482 | ValAcc: 0.8960 | ValLoss: 0.0510 \n",
      "\n",
      "[2017-12-02 18:10:37,603 INFO] Epoch: 483 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:37,907 INFO] Epoch: 483 | ValAcc: 0.8950 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:10:38,528 INFO] Epoch: 484 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:38,838 INFO] Epoch: 484 | ValAcc: 0.8950 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:10:39,471 INFO] Epoch: 485 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:39,801 INFO] Epoch: 485 | ValAcc: 0.8960 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:40,558 INFO] Epoch: 486 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:40,860 INFO] Epoch: 486 | ValAcc: 0.8950 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:41,493 INFO] Epoch: 487 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:41,861 INFO] Epoch: 487 | ValAcc: 0.8950 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:42,502 INFO] Epoch: 488 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:42,809 INFO] Epoch: 488 | ValAcc: 0.8950 | ValLoss: 0.0510 \n",
      "\n",
      "[2017-12-02 18:10:43,447 INFO] Epoch: 489 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:43,758 INFO] Epoch: 489 | ValAcc: 0.8980 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:44,398 INFO] Epoch: 490 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:44,706 INFO] Epoch: 490 | ValAcc: 0.8950 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:45,338 INFO] Epoch: 491 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:45,645 INFO] Epoch: 491 | ValAcc: 0.8960 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:10:46,258 INFO] Epoch: 492 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:46,577 INFO] Epoch: 492 | ValAcc: 0.8960 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:10:47,214 INFO] Epoch: 493 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:47,531 INFO] Epoch: 493 | ValAcc: 0.8960 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:10:48,162 INFO] Epoch: 494 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:48,470 INFO] Epoch: 494 | ValAcc: 0.8950 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:10:49,097 INFO] Epoch: 495 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:49,410 INFO] Epoch: 495 | ValAcc: 0.8960 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:10:50,051 INFO] Epoch: 496 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:50,359 INFO] Epoch: 496 | ValAcc: 0.8950 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:50,998 INFO] Epoch: 497 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:51,326 INFO] Epoch: 497 | ValAcc: 0.8950 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:10:51,961 INFO] Epoch: 498 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:52,270 INFO] Epoch: 498 | ValAcc: 0.8950 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:10:52,893 INFO] Epoch: 499 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:53,196 INFO] Epoch: 499 | ValAcc: 0.8960 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:10:53,836 INFO] Epoch: 500 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:54,163 INFO] Epoch: 500 | ValAcc: 0.8960 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:10:54,792 INFO] Epoch: 501 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:55,097 INFO] Epoch: 501 | ValAcc: 0.8950 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:10:55,746 INFO] Epoch: 502 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:56,059 INFO] Epoch: 502 | ValAcc: 0.8960 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:10:56,668 INFO] Epoch: 503 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:56,975 INFO] Epoch: 503 | ValAcc: 0.8950 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:10:57,603 INFO] Epoch: 504 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:57,913 INFO] Epoch: 504 | ValAcc: 0.8950 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:10:58,529 INFO] Epoch: 505 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:58,837 INFO] Epoch: 505 | ValAcc: 0.8950 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:10:59,466 INFO] Epoch: 506 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:10:59,769 INFO] Epoch: 506 | ValAcc: 0.8960 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:11:00,388 INFO] Epoch: 507 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:00,697 INFO] Epoch: 507 | ValAcc: 0.8950 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:11:01,329 INFO] Epoch: 508 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:01,635 INFO] Epoch: 508 | ValAcc: 0.8960 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:02,255 INFO] Epoch: 509 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:02,568 INFO] Epoch: 509 | ValAcc: 0.8960 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:03,202 INFO] Epoch: 510 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:03,514 INFO] Epoch: 510 | ValAcc: 0.8960 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:11:04,141 INFO] Epoch: 511 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:04,466 INFO] Epoch: 511 | ValAcc: 0.8950 | ValLoss: 0.0511 \n",
      "\n",
      "[2017-12-02 18:11:05,095 INFO] Epoch: 512 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:05,405 INFO] Epoch: 512 | ValAcc: 0.8950 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:11:06,026 INFO] Epoch: 513 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:06,342 INFO] Epoch: 513 | ValAcc: 0.8960 | ValLoss: 0.0512 \n",
      "\n",
      "[2017-12-02 18:11:06,978 INFO] Epoch: 514 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:07,289 INFO] Epoch: 514 | ValAcc: 0.8950 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:11:07,937 INFO] Epoch: 515 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:08,273 INFO] Epoch: 515 | ValAcc: 0.8960 | ValLoss: 0.0513 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:11:08,952 INFO] Epoch: 516 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:09,274 INFO] Epoch: 516 | ValAcc: 0.8970 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:09,918 INFO] Epoch: 517 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:10,225 INFO] Epoch: 517 | ValAcc: 0.8960 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:10,841 INFO] Epoch: 518 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:11,157 INFO] Epoch: 518 | ValAcc: 0.8970 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:11,803 INFO] Epoch: 519 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:12,115 INFO] Epoch: 519 | ValAcc: 0.8960 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:11:12,754 INFO] Epoch: 520 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:13,066 INFO] Epoch: 520 | ValAcc: 0.8970 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:11:13,699 INFO] Epoch: 521 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:14,012 INFO] Epoch: 521 | ValAcc: 0.8970 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:14,658 INFO] Epoch: 522 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:14,966 INFO] Epoch: 522 | ValAcc: 0.8970 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:15,589 INFO] Epoch: 523 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:15,896 INFO] Epoch: 523 | ValAcc: 0.8950 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:11:16,519 INFO] Epoch: 524 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:16,828 INFO] Epoch: 524 | ValAcc: 0.8950 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:17,460 INFO] Epoch: 525 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:17,765 INFO] Epoch: 525 | ValAcc: 0.8950 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:18,390 INFO] Epoch: 526 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:18,706 INFO] Epoch: 526 | ValAcc: 0.8950 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:19,330 INFO] Epoch: 527 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:19,642 INFO] Epoch: 527 | ValAcc: 0.8960 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:20,272 INFO] Epoch: 528 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:20,580 INFO] Epoch: 528 | ValAcc: 0.8980 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:21,207 INFO] Epoch: 529 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:21,522 INFO] Epoch: 529 | ValAcc: 0.8980 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:22,163 INFO] Epoch: 530 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:22,470 INFO] Epoch: 530 | ValAcc: 0.8950 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:23,094 INFO] Epoch: 531 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:23,408 INFO] Epoch: 531 | ValAcc: 0.8950 | ValLoss: 0.0513 \n",
      "\n",
      "[2017-12-02 18:11:24,036 INFO] Epoch: 532 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:24,346 INFO] Epoch: 532 | ValAcc: 0.8970 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:24,997 INFO] Epoch: 533 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:25,312 INFO] Epoch: 533 | ValAcc: 0.8970 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:25,941 INFO] Epoch: 534 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:26,253 INFO] Epoch: 534 | ValAcc: 0.8970 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:26,887 INFO] Epoch: 535 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:27,198 INFO] Epoch: 535 | ValAcc: 0.8960 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:27,851 INFO] Epoch: 536 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:28,180 INFO] Epoch: 536 | ValAcc: 0.8960 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:28,814 INFO] Epoch: 537 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:29,124 INFO] Epoch: 537 | ValAcc: 0.8960 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:29,759 INFO] Epoch: 538 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:30,080 INFO] Epoch: 538 | ValAcc: 0.8960 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:30,707 INFO] Epoch: 539 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:31,023 INFO] Epoch: 539 | ValAcc: 0.8950 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:31,641 INFO] Epoch: 540 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:31,946 INFO] Epoch: 540 | ValAcc: 0.8950 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:32,581 INFO] Epoch: 541 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:32,887 INFO] Epoch: 541 | ValAcc: 0.8950 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:33,524 INFO] Epoch: 542 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:33,836 INFO] Epoch: 542 | ValAcc: 0.8950 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:34,469 INFO] Epoch: 543 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:34,804 INFO] Epoch: 543 | ValAcc: 0.8950 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:35,428 INFO] Epoch: 544 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:35,741 INFO] Epoch: 544 | ValAcc: 0.8960 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:36,370 INFO] Epoch: 545 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:36,679 INFO] Epoch: 545 | ValAcc: 0.8960 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:37,318 INFO] Epoch: 546 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:37,625 INFO] Epoch: 546 | ValAcc: 0.8960 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:38,261 INFO] Epoch: 547 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:38,579 INFO] Epoch: 547 | ValAcc: 0.8950 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:39,214 INFO] Epoch: 548 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:39,520 INFO] Epoch: 548 | ValAcc: 0.8950 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:40,148 INFO] Epoch: 549 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:40,458 INFO] Epoch: 549 | ValAcc: 0.8950 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:41,086 INFO] Epoch: 550 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:41,399 INFO] Epoch: 550 | ValAcc: 0.8960 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:42,037 INFO] Epoch: 551 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:42,354 INFO] Epoch: 551 | ValAcc: 0.8950 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:42,980 INFO] Epoch: 552 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:43,297 INFO] Epoch: 552 | ValAcc: 0.8960 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:11:43,928 INFO] Epoch: 553 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:44,233 INFO] Epoch: 553 | ValAcc: 0.8950 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:44,869 INFO] Epoch: 554 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:45,183 INFO] Epoch: 554 | ValAcc: 0.8950 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:45,819 INFO] Epoch: 555 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:46,126 INFO] Epoch: 555 | ValAcc: 0.8950 | ValLoss: 0.0514 \n",
      "\n",
      "[2017-12-02 18:11:46,755 INFO] Epoch: 556 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:47,067 INFO] Epoch: 556 | ValAcc: 0.8950 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:47,701 INFO] Epoch: 557 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:48,017 INFO] Epoch: 557 | ValAcc: 0.8960 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:48,655 INFO] Epoch: 558 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:48,964 INFO] Epoch: 558 | ValAcc: 0.8950 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:49,599 INFO] Epoch: 559 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:49,913 INFO] Epoch: 559 | ValAcc: 0.8950 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:11:50,555 INFO] Epoch: 560 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:50,870 INFO] Epoch: 560 | ValAcc: 0.8950 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:11:51,503 INFO] Epoch: 561 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:51,816 INFO] Epoch: 561 | ValAcc: 0.8960 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:11:52,437 INFO] Epoch: 562 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:52,733 INFO] Epoch: 562 | ValAcc: 0.8960 | ValLoss: 0.0518 \n",
      "\n",
      "[2017-12-02 18:11:53,387 INFO] Epoch: 563 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:53,699 INFO] Epoch: 563 | ValAcc: 0.8950 | ValLoss: 0.0518 \n",
      "\n",
      "[2017-12-02 18:11:54,331 INFO] Epoch: 564 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:54,643 INFO] Epoch: 564 | ValAcc: 0.8960 | ValLoss: 0.0518 \n",
      "\n",
      "[2017-12-02 18:11:55,299 INFO] Epoch: 565 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:55,614 INFO] Epoch: 565 | ValAcc: 0.8950 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:11:56,245 INFO] Epoch: 566 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:56,565 INFO] Epoch: 566 | ValAcc: 0.8950 | ValLoss: 0.0518 \n",
      "\n",
      "[2017-12-02 18:11:57,205 INFO] Epoch: 567 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:11:57,515 INFO] Epoch: 567 | ValAcc: 0.8950 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:11:58,147 INFO] Epoch: 568 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:58,452 INFO] Epoch: 568 | ValAcc: 0.8960 | ValLoss: 0.0518 \n",
      "\n",
      "[2017-12-02 18:11:59,074 INFO] Epoch: 569 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:11:59,394 INFO] Epoch: 569 | ValAcc: 0.8950 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:12:00,021 INFO] Epoch: 570 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:00,333 INFO] Epoch: 570 | ValAcc: 0.8950 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:12:00,979 INFO] Epoch: 571 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:01,286 INFO] Epoch: 571 | ValAcc: 0.8960 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:12:01,921 INFO] Epoch: 572 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:02,233 INFO] Epoch: 572 | ValAcc: 0.8960 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:12:02,871 INFO] Epoch: 573 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:03,180 INFO] Epoch: 573 | ValAcc: 0.8960 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:12:03,790 INFO] Epoch: 574 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:04,093 INFO] Epoch: 574 | ValAcc: 0.8960 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:12:04,726 INFO] Epoch: 575 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:05,062 INFO] Epoch: 575 | ValAcc: 0.8950 | ValLoss: 0.0515 \n",
      "\n",
      "[2017-12-02 18:12:05,684 INFO] Epoch: 576 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:05,992 INFO] Epoch: 576 | ValAcc: 0.8950 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:12:06,607 INFO] Epoch: 577 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:06,915 INFO] Epoch: 577 | ValAcc: 0.8950 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:12:07,538 INFO] Epoch: 578 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:07,851 INFO] Epoch: 578 | ValAcc: 0.8950 | ValLoss: 0.0516 \n",
      "\n",
      "[2017-12-02 18:12:08,502 INFO] Epoch: 579 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:08,812 INFO] Epoch: 579 | ValAcc: 0.8960 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:12:09,436 INFO] Epoch: 580 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:09,747 INFO] Epoch: 580 | ValAcc: 0.8960 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:12:10,378 INFO] Epoch: 581 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:10,679 INFO] Epoch: 581 | ValAcc: 0.8960 | ValLoss: 0.0517 \n",
      "\n",
      "[2017-12-02 18:12:11,314 INFO] Epoch: 582 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:11,659 INFO] Epoch: 582 | ValAcc: 0.8960 | ValLoss: 0.0518 \n",
      "\n",
      "[2017-12-02 18:12:12,299 INFO] Epoch: 583 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:12,607 INFO] Epoch: 583 | ValAcc: 0.8960 | ValLoss: 0.0518 \n",
      "\n",
      "[2017-12-02 18:12:13,237 INFO] Epoch: 584 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:13,546 INFO] Epoch: 584 | ValAcc: 0.8960 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:14,177 INFO] Epoch: 585 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:14,485 INFO] Epoch: 585 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:15,130 INFO] Epoch: 586 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:15,449 INFO] Epoch: 586 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:16,049 INFO] Epoch: 587 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:16,358 INFO] Epoch: 587 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:16,996 INFO] Epoch: 588 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:17,309 INFO] Epoch: 588 | ValAcc: 0.8960 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:17,951 INFO] Epoch: 589 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:18,270 INFO] Epoch: 589 | ValAcc: 0.8960 | ValLoss: 0.0518 \n",
      "\n",
      "[2017-12-02 18:12:18,887 INFO] Epoch: 590 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:19,201 INFO] Epoch: 590 | ValAcc: 0.8960 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:19,826 INFO] Epoch: 591 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:20,141 INFO] Epoch: 591 | ValAcc: 0.8950 | ValLoss: 0.0518 \n",
      "\n",
      "[2017-12-02 18:12:20,777 INFO] Epoch: 592 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:21,089 INFO] Epoch: 592 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:21,718 INFO] Epoch: 593 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:22,028 INFO] Epoch: 593 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:22,651 INFO] Epoch: 594 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:22,949 INFO] Epoch: 594 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:23,593 INFO] Epoch: 595 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:23,897 INFO] Epoch: 595 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:24,520 INFO] Epoch: 596 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:24,832 INFO] Epoch: 596 | ValAcc: 0.8960 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:25,478 INFO] Epoch: 597 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:25,784 INFO] Epoch: 597 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:26,405 INFO] Epoch: 598 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:26,717 INFO] Epoch: 598 | ValAcc: 0.8950 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:27,362 INFO] Epoch: 599 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:27,677 INFO] Epoch: 599 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:28,309 INFO] Epoch: 600 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:28,613 INFO] Epoch: 600 | ValAcc: 0.8950 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:29,243 INFO] Epoch: 601 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:29,550 INFO] Epoch: 601 | ValAcc: 0.8960 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:30,192 INFO] Epoch: 602 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:30,505 INFO] Epoch: 602 | ValAcc: 0.8960 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:31,151 INFO] Epoch: 603 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:31,456 INFO] Epoch: 603 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:32,087 INFO] Epoch: 604 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:32,400 INFO] Epoch: 604 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:33,012 INFO] Epoch: 605 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:33,332 INFO] Epoch: 605 | ValAcc: 0.8960 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:33,963 INFO] Epoch: 606 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:34,278 INFO] Epoch: 606 | ValAcc: 0.8960 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:34,911 INFO] Epoch: 607 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:35,243 INFO] Epoch: 607 | ValAcc: 0.8950 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:35,863 INFO] Epoch: 608 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:36,184 INFO] Epoch: 608 | ValAcc: 0.8950 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:36,818 INFO] Epoch: 609 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:37,125 INFO] Epoch: 609 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:37,757 INFO] Epoch: 610 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:38,064 INFO] Epoch: 610 | ValAcc: 0.8970 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:12:38,704 INFO] Epoch: 611 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:39,020 INFO] Epoch: 611 | ValAcc: 0.8950 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:39,657 INFO] Epoch: 612 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:39,975 INFO] Epoch: 612 | ValAcc: 0.8960 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:40,612 INFO] Epoch: 613 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:40,920 INFO] Epoch: 613 | ValAcc: 0.8960 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:41,562 INFO] Epoch: 614 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:41,861 INFO] Epoch: 614 | ValAcc: 0.8960 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:42,493 INFO] Epoch: 615 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:42,807 INFO] Epoch: 615 | ValAcc: 0.8960 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:43,447 INFO] Epoch: 616 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:43,758 INFO] Epoch: 616 | ValAcc: 0.8950 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:44,387 INFO] Epoch: 617 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:44,700 INFO] Epoch: 617 | ValAcc: 0.8960 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:45,341 INFO] Epoch: 618 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:45,647 INFO] Epoch: 618 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:12:46,253 INFO] Epoch: 619 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:46,560 INFO] Epoch: 619 | ValAcc: 0.8960 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:47,173 INFO] Epoch: 620 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:47,484 INFO] Epoch: 620 | ValAcc: 0.8950 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:48,105 INFO] Epoch: 621 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:48,416 INFO] Epoch: 621 | ValAcc: 0.8950 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:49,040 INFO] Epoch: 622 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:49,349 INFO] Epoch: 622 | ValAcc: 0.8950 | ValLoss: 0.0519 \n",
      "\n",
      "[2017-12-02 18:12:49,984 INFO] Epoch: 623 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:50,294 INFO] Epoch: 623 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:50,938 INFO] Epoch: 624 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:51,242 INFO] Epoch: 624 | ValAcc: 0.8960 | ValLoss: 0.0520 \n",
      "\n",
      "[2017-12-02 18:12:51,856 INFO] Epoch: 625 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:52,166 INFO] Epoch: 625 | ValAcc: 0.8950 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:52,796 INFO] Epoch: 626 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:53,105 INFO] Epoch: 626 | ValAcc: 0.8960 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:12:53,736 INFO] Epoch: 627 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:54,048 INFO] Epoch: 627 | ValAcc: 0.8960 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:12:54,676 INFO] Epoch: 628 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:54,990 INFO] Epoch: 628 | ValAcc: 0.8950 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:55,623 INFO] Epoch: 629 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:55,932 INFO] Epoch: 629 | ValAcc: 0.8960 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:56,563 INFO] Epoch: 630 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:56,879 INFO] Epoch: 630 | ValAcc: 0.8960 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:57,525 INFO] Epoch: 631 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:57,839 INFO] Epoch: 631 | ValAcc: 0.8960 | ValLoss: 0.0521 \n",
      "\n",
      "[2017-12-02 18:12:58,468 INFO] Epoch: 632 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:58,774 INFO] Epoch: 632 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:12:59,394 INFO] Epoch: 633 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:12:59,698 INFO] Epoch: 633 | ValAcc: 0.8950 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:00,323 INFO] Epoch: 634 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:00,631 INFO] Epoch: 634 | ValAcc: 0.8960 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:01,264 INFO] Epoch: 635 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:01,595 INFO] Epoch: 635 | ValAcc: 0.8960 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:02,219 INFO] Epoch: 636 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:02,530 INFO] Epoch: 636 | ValAcc: 0.8950 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:03,160 INFO] Epoch: 637 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:03,463 INFO] Epoch: 637 | ValAcc: 0.8960 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:04,095 INFO] Epoch: 638 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:04,400 INFO] Epoch: 638 | ValAcc: 0.8960 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:05,034 INFO] Epoch: 639 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:05,340 INFO] Epoch: 639 | ValAcc: 0.8960 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:05,980 INFO] Epoch: 640 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:06,288 INFO] Epoch: 640 | ValAcc: 0.8960 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:06,909 INFO] Epoch: 641 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:07,214 INFO] Epoch: 641 | ValAcc: 0.8950 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:07,862 INFO] Epoch: 642 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:08,179 INFO] Epoch: 642 | ValAcc: 0.8950 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:08,802 INFO] Epoch: 643 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:09,115 INFO] Epoch: 643 | ValAcc: 0.8950 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:09,753 INFO] Epoch: 644 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:10,088 INFO] Epoch: 644 | ValAcc: 0.8950 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:10,726 INFO] Epoch: 645 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:11,033 INFO] Epoch: 645 | ValAcc: 0.8950 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:11,677 INFO] Epoch: 646 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:12,004 INFO] Epoch: 646 | ValAcc: 0.8950 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:12,638 INFO] Epoch: 647 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:12,948 INFO] Epoch: 647 | ValAcc: 0.8950 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:13,587 INFO] Epoch: 648 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:13,903 INFO] Epoch: 648 | ValAcc: 0.8950 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:14,517 INFO] Epoch: 649 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:14,822 INFO] Epoch: 649 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:15,458 INFO] Epoch: 650 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:15,771 INFO] Epoch: 650 | ValAcc: 0.8950 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:16,392 INFO] Epoch: 651 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:16,721 INFO] Epoch: 651 | ValAcc: 0.8950 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:17,341 INFO] Epoch: 652 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:17,646 INFO] Epoch: 652 | ValAcc: 0.8950 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:18,279 INFO] Epoch: 653 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:18,589 INFO] Epoch: 653 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:19,225 INFO] Epoch: 654 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:19,542 INFO] Epoch: 654 | ValAcc: 0.8950 | ValLoss: 0.0522 \n",
      "\n",
      "[2017-12-02 18:13:20,182 INFO] Epoch: 655 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:20,489 INFO] Epoch: 655 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:21,100 INFO] Epoch: 656 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:21,414 INFO] Epoch: 656 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:22,049 INFO] Epoch: 657 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:22,355 INFO] Epoch: 657 | ValAcc: 0.8970 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:22,995 INFO] Epoch: 658 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:23,307 INFO] Epoch: 658 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:23,916 INFO] Epoch: 659 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:24,223 INFO] Epoch: 659 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:24,852 INFO] Epoch: 660 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:25,160 INFO] Epoch: 660 | ValAcc: 0.8950 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:25,810 INFO] Epoch: 661 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:26,123 INFO] Epoch: 661 | ValAcc: 0.8950 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:26,736 INFO] Epoch: 662 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:27,041 INFO] Epoch: 662 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:27,670 INFO] Epoch: 663 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:27,984 INFO] Epoch: 663 | ValAcc: 0.8950 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:28,632 INFO] Epoch: 664 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:28,945 INFO] Epoch: 664 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:29,570 INFO] Epoch: 665 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:29,884 INFO] Epoch: 665 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:30,523 INFO] Epoch: 666 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:30,828 INFO] Epoch: 666 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:31,450 INFO] Epoch: 667 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:31,754 INFO] Epoch: 667 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:32,387 INFO] Epoch: 668 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:32,696 INFO] Epoch: 668 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:33,338 INFO] Epoch: 669 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:33,650 INFO] Epoch: 669 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:34,285 INFO] Epoch: 670 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:13:34,593 INFO] Epoch: 670 | ValAcc: 0.8970 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:35,236 INFO] Epoch: 671 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:35,547 INFO] Epoch: 671 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:36,165 INFO] Epoch: 672 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:36,475 INFO] Epoch: 672 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:37,089 INFO] Epoch: 673 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:37,389 INFO] Epoch: 673 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:38,016 INFO] Epoch: 674 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:38,327 INFO] Epoch: 674 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:38,959 INFO] Epoch: 675 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:39,272 INFO] Epoch: 675 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:39,915 INFO] Epoch: 676 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:40,229 INFO] Epoch: 676 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:40,866 INFO] Epoch: 677 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:41,178 INFO] Epoch: 677 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:41,810 INFO] Epoch: 678 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:42,129 INFO] Epoch: 678 | ValAcc: 0.8950 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:42,761 INFO] Epoch: 679 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:43,071 INFO] Epoch: 679 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:43,705 INFO] Epoch: 680 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:44,016 INFO] Epoch: 680 | ValAcc: 0.8950 | ValLoss: 0.0525 \n",
      "\n",
      "[2017-12-02 18:13:44,648 INFO] Epoch: 681 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:44,968 INFO] Epoch: 681 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:45,602 INFO] Epoch: 682 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:45,928 INFO] Epoch: 682 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:46,557 INFO] Epoch: 683 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:46,868 INFO] Epoch: 683 | ValAcc: 0.8960 | ValLoss: 0.0523 \n",
      "\n",
      "[2017-12-02 18:13:47,511 INFO] Epoch: 684 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:47,848 INFO] Epoch: 684 | ValAcc: 0.8960 | ValLoss: 0.0525 \n",
      "\n",
      "[2017-12-02 18:13:48,484 INFO] Epoch: 685 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:48,790 INFO] Epoch: 685 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:49,434 INFO] Epoch: 686 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:49,741 INFO] Epoch: 686 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:50,364 INFO] Epoch: 687 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:50,683 INFO] Epoch: 687 | ValAcc: 0.8950 | ValLoss: 0.0525 \n",
      "\n",
      "[2017-12-02 18:13:51,321 INFO] Epoch: 688 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:51,631 INFO] Epoch: 688 | ValAcc: 0.8950 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:13:52,277 INFO] Epoch: 689 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:52,585 INFO] Epoch: 689 | ValAcc: 0.8950 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:53,239 INFO] Epoch: 690 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:53,580 INFO] Epoch: 690 | ValAcc: 0.8950 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:13:54,210 INFO] Epoch: 691 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:54,518 INFO] Epoch: 691 | ValAcc: 0.8960 | ValLoss: 0.0525 \n",
      "\n",
      "[2017-12-02 18:13:55,157 INFO] Epoch: 692 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:55,492 INFO] Epoch: 692 | ValAcc: 0.8960 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:13:56,139 INFO] Epoch: 693 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:56,458 INFO] Epoch: 693 | ValAcc: 0.8950 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:13:57,097 INFO] Epoch: 694 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:57,410 INFO] Epoch: 694 | ValAcc: 0.8950 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:13:58,047 INFO] Epoch: 695 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:58,366 INFO] Epoch: 695 | ValAcc: 0.8960 | ValLoss: 0.0524 \n",
      "\n",
      "[2017-12-02 18:13:58,984 INFO] Epoch: 696 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:13:59,300 INFO] Epoch: 696 | ValAcc: 0.8970 | ValLoss: 0.0525 \n",
      "\n",
      "[2017-12-02 18:13:59,917 INFO] Epoch: 697 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:00,236 INFO] Epoch: 697 | ValAcc: 0.8970 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:00,870 INFO] Epoch: 698 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:01,184 INFO] Epoch: 698 | ValAcc: 0.8960 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:01,815 INFO] Epoch: 699 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:02,126 INFO] Epoch: 699 | ValAcc: 0.8950 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:02,753 INFO] Epoch: 700 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:03,060 INFO] Epoch: 700 | ValAcc: 0.8950 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:03,696 INFO] Epoch: 701 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:04,003 INFO] Epoch: 701 | ValAcc: 0.8960 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:04,622 INFO] Epoch: 702 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:04,927 INFO] Epoch: 702 | ValAcc: 0.8950 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:05,565 INFO] Epoch: 703 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:05,877 INFO] Epoch: 703 | ValAcc: 0.8950 | ValLoss: 0.0525 \n",
      "\n",
      "[2017-12-02 18:14:06,519 INFO] Epoch: 704 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:06,828 INFO] Epoch: 704 | ValAcc: 0.8960 | ValLoss: 0.0525 \n",
      "\n",
      "[2017-12-02 18:14:07,452 INFO] Epoch: 705 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:07,760 INFO] Epoch: 705 | ValAcc: 0.8950 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:08,382 INFO] Epoch: 706 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:08,697 INFO] Epoch: 706 | ValAcc: 0.8960 | ValLoss: 0.0525 \n",
      "\n",
      "[2017-12-02 18:14:09,340 INFO] Epoch: 707 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:09,645 INFO] Epoch: 707 | ValAcc: 0.8950 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:10,265 INFO] Epoch: 708 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:10,574 INFO] Epoch: 708 | ValAcc: 0.8950 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:11,218 INFO] Epoch: 709 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:11,548 INFO] Epoch: 709 | ValAcc: 0.8950 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:12,198 INFO] Epoch: 710 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:12,518 INFO] Epoch: 710 | ValAcc: 0.8960 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:13,150 INFO] Epoch: 711 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:13,464 INFO] Epoch: 711 | ValAcc: 0.8960 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:14,085 INFO] Epoch: 712 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:14,393 INFO] Epoch: 712 | ValAcc: 0.8960 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:15,031 INFO] Epoch: 713 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:15,340 INFO] Epoch: 713 | ValAcc: 0.8950 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:15,968 INFO] Epoch: 714 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:16,289 INFO] Epoch: 714 | ValAcc: 0.8970 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:16,906 INFO] Epoch: 715 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:17,212 INFO] Epoch: 715 | ValAcc: 0.8970 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:17,848 INFO] Epoch: 716 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:18,159 INFO] Epoch: 716 | ValAcc: 0.8960 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:18,792 INFO] Epoch: 717 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:19,096 INFO] Epoch: 717 | ValAcc: 0.8960 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:19,730 INFO] Epoch: 718 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:20,039 INFO] Epoch: 718 | ValAcc: 0.8970 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:20,674 INFO] Epoch: 719 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:20,982 INFO] Epoch: 719 | ValAcc: 0.8960 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:21,618 INFO] Epoch: 720 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:21,941 INFO] Epoch: 720 | ValAcc: 0.8980 | ValLoss: 0.0526 \n",
      "\n",
      "[2017-12-02 18:14:22,576 INFO] Epoch: 721 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:22,888 INFO] Epoch: 721 | ValAcc: 0.8970 | ValLoss: 0.0525 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:14:23,531 INFO] Epoch: 722 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:23,843 INFO] Epoch: 722 | ValAcc: 0.8960 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:24,488 INFO] Epoch: 723 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:24,791 INFO] Epoch: 723 | ValAcc: 0.8960 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:25,420 INFO] Epoch: 724 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:25,737 INFO] Epoch: 724 | ValAcc: 0.8960 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:26,388 INFO] Epoch: 725 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:26,691 INFO] Epoch: 725 | ValAcc: 0.8960 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:27,330 INFO] Epoch: 726 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:27,641 INFO] Epoch: 726 | ValAcc: 0.8960 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:28,277 INFO] Epoch: 727 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:28,582 INFO] Epoch: 727 | ValAcc: 0.8950 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:29,210 INFO] Epoch: 728 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:29,520 INFO] Epoch: 728 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:14:30,155 INFO] Epoch: 729 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:30,483 INFO] Epoch: 729 | ValAcc: 0.8960 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:31,097 INFO] Epoch: 730 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:31,406 INFO] Epoch: 730 | ValAcc: 0.8960 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:32,021 INFO] Epoch: 731 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:32,323 INFO] Epoch: 731 | ValAcc: 0.8960 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:32,939 INFO] Epoch: 732 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:33,243 INFO] Epoch: 732 | ValAcc: 0.8970 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:33,859 INFO] Epoch: 733 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:34,171 INFO] Epoch: 733 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:14:34,805 INFO] Epoch: 734 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:35,114 INFO] Epoch: 734 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:14:35,745 INFO] Epoch: 735 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:36,050 INFO] Epoch: 735 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:14:36,703 INFO] Epoch: 736 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:37,030 INFO] Epoch: 736 | ValAcc: 0.8970 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:37,673 INFO] Epoch: 737 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:37,986 INFO] Epoch: 737 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:14:38,622 INFO] Epoch: 738 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:38,935 INFO] Epoch: 738 | ValAcc: 0.8970 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:39,569 INFO] Epoch: 739 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:39,875 INFO] Epoch: 739 | ValAcc: 0.8960 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:40,510 INFO] Epoch: 740 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:40,813 INFO] Epoch: 740 | ValAcc: 0.8970 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:41,434 INFO] Epoch: 741 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:41,752 INFO] Epoch: 741 | ValAcc: 0.8960 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:42,390 INFO] Epoch: 742 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:42,702 INFO] Epoch: 742 | ValAcc: 0.8970 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:43,330 INFO] Epoch: 743 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:43,644 INFO] Epoch: 743 | ValAcc: 0.8970 | ValLoss: 0.0527 \n",
      "\n",
      "[2017-12-02 18:14:44,273 INFO] Epoch: 744 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:44,578 INFO] Epoch: 744 | ValAcc: 0.8960 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:14:45,197 INFO] Epoch: 745 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:45,498 INFO] Epoch: 745 | ValAcc: 0.8970 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:46,115 INFO] Epoch: 746 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:46,449 INFO] Epoch: 746 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:14:47,096 INFO] Epoch: 747 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:47,413 INFO] Epoch: 747 | ValAcc: 0.8960 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:48,056 INFO] Epoch: 748 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:48,368 INFO] Epoch: 748 | ValAcc: 0.8960 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:14:49,008 INFO] Epoch: 749 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:49,317 INFO] Epoch: 749 | ValAcc: 0.8960 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:14:49,962 INFO] Epoch: 750 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:50,270 INFO] Epoch: 750 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:14:50,888 INFO] Epoch: 751 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:51,205 INFO] Epoch: 751 | ValAcc: 0.8970 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:51,841 INFO] Epoch: 752 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:52,159 INFO] Epoch: 752 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:14:52,784 INFO] Epoch: 753 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:53,098 INFO] Epoch: 753 | ValAcc: 0.8950 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:14:53,729 INFO] Epoch: 754 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:54,035 INFO] Epoch: 754 | ValAcc: 0.8950 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:14:54,653 INFO] Epoch: 755 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:54,969 INFO] Epoch: 755 | ValAcc: 0.8950 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:14:55,605 INFO] Epoch: 756 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:55,911 INFO] Epoch: 756 | ValAcc: 0.8950 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:14:56,547 INFO] Epoch: 757 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:56,848 INFO] Epoch: 757 | ValAcc: 0.8960 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:14:57,492 INFO] Epoch: 758 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:57,800 INFO] Epoch: 758 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:14:58,439 INFO] Epoch: 759 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:58,759 INFO] Epoch: 759 | ValAcc: 0.8980 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:14:59,399 INFO] Epoch: 760 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:14:59,714 INFO] Epoch: 760 | ValAcc: 0.8970 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:15:00,333 INFO] Epoch: 761 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:00,641 INFO] Epoch: 761 | ValAcc: 0.8970 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:15:01,271 INFO] Epoch: 762 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:01,577 INFO] Epoch: 762 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:15:02,215 INFO] Epoch: 763 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:02,518 INFO] Epoch: 763 | ValAcc: 0.8960 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:03,178 INFO] Epoch: 764 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:03,492 INFO] Epoch: 764 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:15:04,121 INFO] Epoch: 765 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:04,432 INFO] Epoch: 765 | ValAcc: 0.8960 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:15:05,064 INFO] Epoch: 766 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:05,377 INFO] Epoch: 766 | ValAcc: 0.8960 | ValLoss: 0.0528 \n",
      "\n",
      "[2017-12-02 18:15:05,994 INFO] Epoch: 767 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:06,302 INFO] Epoch: 767 | ValAcc: 0.8970 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:15:06,941 INFO] Epoch: 768 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:07,257 INFO] Epoch: 768 | ValAcc: 0.8970 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:15:07,896 INFO] Epoch: 769 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:08,208 INFO] Epoch: 769 | ValAcc: 0.8980 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:15:08,825 INFO] Epoch: 770 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:09,139 INFO] Epoch: 770 | ValAcc: 0.8980 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:09,764 INFO] Epoch: 771 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:10,073 INFO] Epoch: 771 | ValAcc: 0.8960 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:10,690 INFO] Epoch: 772 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:11,043 INFO] Epoch: 772 | ValAcc: 0.8960 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:11,715 INFO] Epoch: 773 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:15:12,032 INFO] Epoch: 773 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:12,664 INFO] Epoch: 774 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:12,972 INFO] Epoch: 774 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:13,605 INFO] Epoch: 775 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:13,928 INFO] Epoch: 775 | ValAcc: 0.8960 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:14,541 INFO] Epoch: 776 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:14,850 INFO] Epoch: 776 | ValAcc: 0.8980 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:15,471 INFO] Epoch: 777 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:15,778 INFO] Epoch: 777 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:16,413 INFO] Epoch: 778 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:16,739 INFO] Epoch: 778 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:17,365 INFO] Epoch: 779 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:17,674 INFO] Epoch: 779 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:18,313 INFO] Epoch: 780 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:18,627 INFO] Epoch: 780 | ValAcc: 0.8960 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:19,259 INFO] Epoch: 781 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:19,572 INFO] Epoch: 781 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:20,202 INFO] Epoch: 782 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:20,522 INFO] Epoch: 782 | ValAcc: 0.8960 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:21,152 INFO] Epoch: 783 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:21,464 INFO] Epoch: 783 | ValAcc: 0.8980 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:22,101 INFO] Epoch: 784 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:22,409 INFO] Epoch: 784 | ValAcc: 0.8960 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:23,041 INFO] Epoch: 785 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:23,350 INFO] Epoch: 785 | ValAcc: 0.8960 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:15:23,964 INFO] Epoch: 786 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:24,283 INFO] Epoch: 786 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:24,905 INFO] Epoch: 787 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:25,217 INFO] Epoch: 787 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:25,849 INFO] Epoch: 788 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:26,160 INFO] Epoch: 788 | ValAcc: 0.8980 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:26,811 INFO] Epoch: 789 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:27,128 INFO] Epoch: 789 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:27,767 INFO] Epoch: 790 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:28,080 INFO] Epoch: 790 | ValAcc: 0.8980 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:28,707 INFO] Epoch: 791 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:29,018 INFO] Epoch: 791 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:29,663 INFO] Epoch: 792 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:29,972 INFO] Epoch: 792 | ValAcc: 0.8960 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:30,615 INFO] Epoch: 793 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:30,925 INFO] Epoch: 793 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:31,561 INFO] Epoch: 794 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:31,872 INFO] Epoch: 794 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:32,484 INFO] Epoch: 795 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:32,787 INFO] Epoch: 795 | ValAcc: 0.8970 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:15:33,419 INFO] Epoch: 796 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:33,723 INFO] Epoch: 796 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:34,354 INFO] Epoch: 797 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:34,662 INFO] Epoch: 797 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:35,311 INFO] Epoch: 798 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:35,629 INFO] Epoch: 798 | ValAcc: 0.8970 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:36,265 INFO] Epoch: 799 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:36,611 INFO] Epoch: 799 | ValAcc: 0.8970 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:37,344 INFO] Epoch: 800 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:37,741 INFO] Epoch: 800 | ValAcc: 0.8970 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:38,446 INFO] Epoch: 801 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:38,804 INFO] Epoch: 801 | ValAcc: 0.8960 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:15:39,531 INFO] Epoch: 802 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:39,954 INFO] Epoch: 802 | ValAcc: 0.8960 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:15:40,889 INFO] Epoch: 803 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:41,306 INFO] Epoch: 803 | ValAcc: 0.8970 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:15:42,103 INFO] Epoch: 804 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:42,469 INFO] Epoch: 804 | ValAcc: 0.8970 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:15:43,211 INFO] Epoch: 805 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:43,576 INFO] Epoch: 805 | ValAcc: 0.8970 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:44,326 INFO] Epoch: 806 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:44,653 INFO] Epoch: 806 | ValAcc: 0.8980 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:15:45,383 INFO] Epoch: 807 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:45,751 INFO] Epoch: 807 | ValAcc: 0.8970 | ValLoss: 0.0530 \n",
      "\n",
      "[2017-12-02 18:15:46,442 INFO] Epoch: 808 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:46,797 INFO] Epoch: 808 | ValAcc: 0.8970 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:47,680 INFO] Epoch: 809 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:48,117 INFO] Epoch: 809 | ValAcc: 0.8970 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:48,956 INFO] Epoch: 810 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:49,266 INFO] Epoch: 810 | ValAcc: 0.8980 | ValLoss: 0.0529 \n",
      "\n",
      "[2017-12-02 18:15:49,975 INFO] Epoch: 811 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:50,349 INFO] Epoch: 811 | ValAcc: 0.8980 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:51,184 INFO] Epoch: 812 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:51,592 INFO] Epoch: 812 | ValAcc: 0.8980 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:15:52,349 INFO] Epoch: 813 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:52,761 INFO] Epoch: 813 | ValAcc: 0.8970 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:15:53,497 INFO] Epoch: 814 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:53,833 INFO] Epoch: 814 | ValAcc: 0.8980 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:15:54,648 INFO] Epoch: 815 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:55,008 INFO] Epoch: 815 | ValAcc: 0.8980 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:55,815 INFO] Epoch: 816 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:56,141 INFO] Epoch: 816 | ValAcc: 0.8980 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:56,812 INFO] Epoch: 817 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:57,142 INFO] Epoch: 817 | ValAcc: 0.8970 | ValLoss: 0.0531 \n",
      "\n",
      "[2017-12-02 18:15:57,794 INFO] Epoch: 818 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:58,103 INFO] Epoch: 818 | ValAcc: 0.8980 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:15:58,837 INFO] Epoch: 819 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:15:59,243 INFO] Epoch: 819 | ValAcc: 0.8980 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:15:59,895 INFO] Epoch: 820 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:00,215 INFO] Epoch: 820 | ValAcc: 0.8970 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:00,961 INFO] Epoch: 821 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:01,361 INFO] Epoch: 821 | ValAcc: 0.8980 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:02,022 INFO] Epoch: 822 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:02,344 INFO] Epoch: 822 | ValAcc: 0.8980 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:02,973 INFO] Epoch: 823 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:03,282 INFO] Epoch: 823 | ValAcc: 0.8970 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:16:03,925 INFO] Epoch: 824 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:04,236 INFO] Epoch: 824 | ValAcc: 0.8980 | ValLoss: 0.0533 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:16:04,845 INFO] Epoch: 825 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:05,159 INFO] Epoch: 825 | ValAcc: 0.8960 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:05,775 INFO] Epoch: 826 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:06,079 INFO] Epoch: 826 | ValAcc: 0.8970 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:06,696 INFO] Epoch: 827 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:07,005 INFO] Epoch: 827 | ValAcc: 0.8980 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:16:07,648 INFO] Epoch: 828 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:07,957 INFO] Epoch: 828 | ValAcc: 0.8970 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:08,589 INFO] Epoch: 829 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:08,893 INFO] Epoch: 829 | ValAcc: 0.8970 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:09,512 INFO] Epoch: 830 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:09,825 INFO] Epoch: 830 | ValAcc: 0.8980 | ValLoss: 0.0532 \n",
      "\n",
      "[2017-12-02 18:16:10,446 INFO] Epoch: 831 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:10,751 INFO] Epoch: 831 | ValAcc: 0.8970 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:11,385 INFO] Epoch: 832 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:11,712 INFO] Epoch: 832 | ValAcc: 0.8980 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:12,342 INFO] Epoch: 833 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:12,646 INFO] Epoch: 833 | ValAcc: 0.8980 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:13,271 INFO] Epoch: 834 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:13,580 INFO] Epoch: 834 | ValAcc: 0.8980 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:14,208 INFO] Epoch: 835 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:14,510 INFO] Epoch: 835 | ValAcc: 0.8970 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:15,129 INFO] Epoch: 836 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:15,439 INFO] Epoch: 836 | ValAcc: 0.8970 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:16,061 INFO] Epoch: 837 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:16,371 INFO] Epoch: 837 | ValAcc: 0.8980 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:16,982 INFO] Epoch: 838 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:17,297 INFO] Epoch: 838 | ValAcc: 0.8980 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:17,924 INFO] Epoch: 839 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:18,228 INFO] Epoch: 839 | ValAcc: 0.8970 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:18,856 INFO] Epoch: 840 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:19,166 INFO] Epoch: 840 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:19,806 INFO] Epoch: 841 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:20,115 INFO] Epoch: 841 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:20,737 INFO] Epoch: 842 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:21,044 INFO] Epoch: 842 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:21,669 INFO] Epoch: 843 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:21,978 INFO] Epoch: 843 | ValAcc: 0.8970 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:22,609 INFO] Epoch: 844 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:22,922 INFO] Epoch: 844 | ValAcc: 0.8980 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:23,563 INFO] Epoch: 845 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:23,870 INFO] Epoch: 845 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:24,489 INFO] Epoch: 846 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:24,795 INFO] Epoch: 846 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:25,432 INFO] Epoch: 847 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:25,741 INFO] Epoch: 847 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:26,355 INFO] Epoch: 848 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:26,656 INFO] Epoch: 848 | ValAcc: 0.8970 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:27,276 INFO] Epoch: 849 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:27,596 INFO] Epoch: 849 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:28,243 INFO] Epoch: 850 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:28,564 INFO] Epoch: 850 | ValAcc: 0.8970 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:29,179 INFO] Epoch: 851 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:29,485 INFO] Epoch: 851 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:30,132 INFO] Epoch: 852 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:30,444 INFO] Epoch: 852 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:31,074 INFO] Epoch: 853 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:31,387 INFO] Epoch: 853 | ValAcc: 0.8970 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:32,026 INFO] Epoch: 854 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:32,335 INFO] Epoch: 854 | ValAcc: 0.8970 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:32,962 INFO] Epoch: 855 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:33,275 INFO] Epoch: 855 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:33,896 INFO] Epoch: 856 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:34,222 INFO] Epoch: 856 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:34,860 INFO] Epoch: 857 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:35,171 INFO] Epoch: 857 | ValAcc: 0.8980 | ValLoss: 0.0533 \n",
      "\n",
      "[2017-12-02 18:16:35,797 INFO] Epoch: 858 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:36,100 INFO] Epoch: 858 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:36,728 INFO] Epoch: 859 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:37,029 INFO] Epoch: 859 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:37,673 INFO] Epoch: 860 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:37,978 INFO] Epoch: 860 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:38,600 INFO] Epoch: 861 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:38,906 INFO] Epoch: 861 | ValAcc: 0.8980 | ValLoss: 0.0534 \n",
      "\n",
      "[2017-12-02 18:16:39,530 INFO] Epoch: 862 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:39,842 INFO] Epoch: 862 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:40,471 INFO] Epoch: 863 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:40,794 INFO] Epoch: 863 | ValAcc: 0.8960 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:41,420 INFO] Epoch: 864 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:41,737 INFO] Epoch: 864 | ValAcc: 0.8970 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:42,376 INFO] Epoch: 865 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:42,679 INFO] Epoch: 865 | ValAcc: 0.8970 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:43,277 INFO] Epoch: 866 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:43,588 INFO] Epoch: 866 | ValAcc: 0.8970 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:44,210 INFO] Epoch: 867 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:44,524 INFO] Epoch: 867 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:45,137 INFO] Epoch: 868 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:45,447 INFO] Epoch: 868 | ValAcc: 0.8970 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:46,082 INFO] Epoch: 869 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:46,387 INFO] Epoch: 869 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:47,026 INFO] Epoch: 870 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:47,345 INFO] Epoch: 870 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:47,991 INFO] Epoch: 871 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:48,315 INFO] Epoch: 871 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:49,112 INFO] Epoch: 872 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:49,549 INFO] Epoch: 872 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:50,363 INFO] Epoch: 873 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:50,726 INFO] Epoch: 873 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:51,532 INFO] Epoch: 874 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:51,867 INFO] Epoch: 874 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:52,553 INFO] Epoch: 875 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:52,895 INFO] Epoch: 875 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:16:53,522 INFO] Epoch: 876 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:16:53,844 INFO] Epoch: 876 | ValAcc: 0.8970 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:54,602 INFO] Epoch: 877 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:54,904 INFO] Epoch: 877 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:55,544 INFO] Epoch: 878 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:55,851 INFO] Epoch: 878 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:16:56,531 INFO] Epoch: 879 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:56,841 INFO] Epoch: 879 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:57,491 INFO] Epoch: 880 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:57,808 INFO] Epoch: 880 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:16:58,435 INFO] Epoch: 881 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:58,725 INFO] Epoch: 881 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:16:59,350 INFO] Epoch: 882 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:16:59,653 INFO] Epoch: 882 | ValAcc: 0.8970 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:00,274 INFO] Epoch: 883 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:00,599 INFO] Epoch: 883 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:01,237 INFO] Epoch: 884 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:01,534 INFO] Epoch: 884 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:02,165 INFO] Epoch: 885 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:02,481 INFO] Epoch: 885 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:17:03,125 INFO] Epoch: 886 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:03,473 INFO] Epoch: 886 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:17:04,103 INFO] Epoch: 887 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:04,418 INFO] Epoch: 887 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:05,098 INFO] Epoch: 888 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:05,415 INFO] Epoch: 888 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:06,029 INFO] Epoch: 889 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:06,349 INFO] Epoch: 889 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:06,979 INFO] Epoch: 890 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:07,285 INFO] Epoch: 890 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:07,925 INFO] Epoch: 891 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:08,230 INFO] Epoch: 891 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:08,867 INFO] Epoch: 892 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:09,178 INFO] Epoch: 892 | ValAcc: 0.8980 | ValLoss: 0.0535 \n",
      "\n",
      "[2017-12-02 18:17:09,806 INFO] Epoch: 893 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:10,122 INFO] Epoch: 893 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:10,776 INFO] Epoch: 894 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:11,107 INFO] Epoch: 894 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:11,751 INFO] Epoch: 895 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:12,070 INFO] Epoch: 895 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:12,702 INFO] Epoch: 896 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:13,012 INFO] Epoch: 896 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:13,651 INFO] Epoch: 897 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:13,953 INFO] Epoch: 897 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:14,584 INFO] Epoch: 898 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:14,890 INFO] Epoch: 898 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:15,542 INFO] Epoch: 899 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:15,856 INFO] Epoch: 899 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:16,506 INFO] Epoch: 900 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:16,802 INFO] Epoch: 900 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:17,443 INFO] Epoch: 901 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:17,748 INFO] Epoch: 901 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:18,385 INFO] Epoch: 902 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:18,688 INFO] Epoch: 902 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:19,309 INFO] Epoch: 903 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:19,598 INFO] Epoch: 903 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:20,208 INFO] Epoch: 904 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:20,516 INFO] Epoch: 904 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:21,125 INFO] Epoch: 905 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:21,422 INFO] Epoch: 905 | ValAcc: 0.8970 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:22,038 INFO] Epoch: 906 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:22,341 INFO] Epoch: 906 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:22,959 INFO] Epoch: 907 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:23,260 INFO] Epoch: 907 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:23,882 INFO] Epoch: 908 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:24,182 INFO] Epoch: 908 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:24,796 INFO] Epoch: 909 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:25,089 INFO] Epoch: 909 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:25,699 INFO] Epoch: 910 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:25,993 INFO] Epoch: 910 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:26,621 INFO] Epoch: 911 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:26,923 INFO] Epoch: 911 | ValAcc: 0.8970 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:27,538 INFO] Epoch: 912 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:27,835 INFO] Epoch: 912 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:28,463 INFO] Epoch: 913 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:28,770 INFO] Epoch: 913 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:29,457 INFO] Epoch: 914 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:29,825 INFO] Epoch: 914 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:30,487 INFO] Epoch: 915 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:30,810 INFO] Epoch: 915 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:31,446 INFO] Epoch: 916 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:31,745 INFO] Epoch: 916 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:32,378 INFO] Epoch: 917 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:32,696 INFO] Epoch: 917 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:33,366 INFO] Epoch: 918 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:33,743 INFO] Epoch: 918 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:34,461 INFO] Epoch: 919 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:34,775 INFO] Epoch: 919 | ValAcc: 0.8970 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:35,569 INFO] Epoch: 920 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:35,933 INFO] Epoch: 920 | ValAcc: 0.8970 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:36,547 INFO] Epoch: 921 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:36,863 INFO] Epoch: 921 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:37,483 INFO] Epoch: 922 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:37,790 INFO] Epoch: 922 | ValAcc: 0.8980 | ValLoss: 0.0536 \n",
      "\n",
      "[2017-12-02 18:17:38,435 INFO] Epoch: 923 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:38,739 INFO] Epoch: 923 | ValAcc: 0.8980 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:39,362 INFO] Epoch: 924 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:39,677 INFO] Epoch: 924 | ValAcc: 0.8970 | ValLoss: 0.0537 \n",
      "\n",
      "[2017-12-02 18:17:40,352 INFO] Epoch: 925 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:40,747 INFO] Epoch: 925 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:41,383 INFO] Epoch: 926 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:41,742 INFO] Epoch: 926 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:42,564 INFO] Epoch: 927 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:42,877 INFO] Epoch: 927 | ValAcc: 0.8970 | ValLoss: 0.0539 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:17:43,644 INFO] Epoch: 928 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:43,950 INFO] Epoch: 928 | ValAcc: 0.8970 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:44,718 INFO] Epoch: 929 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:45,040 INFO] Epoch: 929 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:45,642 INFO] Epoch: 930 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:45,949 INFO] Epoch: 930 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:46,550 INFO] Epoch: 931 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:46,913 INFO] Epoch: 931 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:47,613 INFO] Epoch: 932 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:47,926 INFO] Epoch: 932 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:48,564 INFO] Epoch: 933 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:48,873 INFO] Epoch: 933 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:49,495 INFO] Epoch: 934 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:49,803 INFO] Epoch: 934 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:50,460 INFO] Epoch: 935 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:50,762 INFO] Epoch: 935 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:17:51,384 INFO] Epoch: 936 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:51,690 INFO] Epoch: 936 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:52,319 INFO] Epoch: 937 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:52,633 INFO] Epoch: 937 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:53,263 INFO] Epoch: 938 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:53,602 INFO] Epoch: 938 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:54,442 INFO] Epoch: 939 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:54,742 INFO] Epoch: 939 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:17:55,371 INFO] Epoch: 940 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:55,677 INFO] Epoch: 940 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:56,281 INFO] Epoch: 941 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:56,595 INFO] Epoch: 941 | ValAcc: 0.8980 | ValLoss: 0.0538 \n",
      "\n",
      "[2017-12-02 18:17:57,194 INFO] Epoch: 942 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:57,508 INFO] Epoch: 942 | ValAcc: 0.8970 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:17:58,130 INFO] Epoch: 943 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:58,459 INFO] Epoch: 943 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:17:59,184 INFO] Epoch: 944 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:17:59,493 INFO] Epoch: 944 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:00,109 INFO] Epoch: 945 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:00,417 INFO] Epoch: 945 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:01,124 INFO] Epoch: 946 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:01,531 INFO] Epoch: 946 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:02,167 INFO] Epoch: 947 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:02,471 INFO] Epoch: 947 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:18:03,092 INFO] Epoch: 948 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:03,393 INFO] Epoch: 948 | ValAcc: 0.8970 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:18:04,079 INFO] Epoch: 949 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:04,409 INFO] Epoch: 949 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:18:05,023 INFO] Epoch: 950 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:05,352 INFO] Epoch: 950 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:05,972 INFO] Epoch: 951 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:06,278 INFO] Epoch: 951 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:06,915 INFO] Epoch: 952 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:07,215 INFO] Epoch: 952 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:07,846 INFO] Epoch: 953 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:08,148 INFO] Epoch: 953 | ValAcc: 0.8980 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:18:08,792 INFO] Epoch: 954 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:09,089 INFO] Epoch: 954 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:09,698 INFO] Epoch: 955 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:09,996 INFO] Epoch: 955 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:10,614 INFO] Epoch: 956 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:10,912 INFO] Epoch: 956 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:11,550 INFO] Epoch: 957 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:11,872 INFO] Epoch: 957 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:12,489 INFO] Epoch: 958 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:12,790 INFO] Epoch: 958 | ValAcc: 0.8970 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:18:13,428 INFO] Epoch: 959 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:13,734 INFO] Epoch: 959 | ValAcc: 0.8970 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:18:14,349 INFO] Epoch: 960 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:14,656 INFO] Epoch: 960 | ValAcc: 0.8970 | ValLoss: 0.0539 \n",
      "\n",
      "[2017-12-02 18:18:15,278 INFO] Epoch: 961 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:15,576 INFO] Epoch: 961 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:16,183 INFO] Epoch: 962 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:16,488 INFO] Epoch: 962 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:17,116 INFO] Epoch: 963 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:17,410 INFO] Epoch: 963 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:18,035 INFO] Epoch: 964 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:18,346 INFO] Epoch: 964 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:18,974 INFO] Epoch: 965 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:19,281 INFO] Epoch: 965 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:19,881 INFO] Epoch: 966 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:20,179 INFO] Epoch: 966 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:20,797 INFO] Epoch: 967 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:21,094 INFO] Epoch: 967 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:21,702 INFO] Epoch: 968 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:21,995 INFO] Epoch: 968 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:22,578 INFO] Epoch: 969 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:22,877 INFO] Epoch: 969 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:23,479 INFO] Epoch: 970 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:23,785 INFO] Epoch: 970 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:24,411 INFO] Epoch: 971 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:24,715 INFO] Epoch: 971 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:25,343 INFO] Epoch: 972 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:25,646 INFO] Epoch: 972 | ValAcc: 0.8970 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:26,250 INFO] Epoch: 973 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:26,551 INFO] Epoch: 973 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:27,172 INFO] Epoch: 974 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:27,481 INFO] Epoch: 974 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:28,109 INFO] Epoch: 975 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:28,413 INFO] Epoch: 975 | ValAcc: 0.8980 | ValLoss: 0.0542 \n",
      "\n",
      "[2017-12-02 18:18:29,037 INFO] Epoch: 976 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:29,345 INFO] Epoch: 976 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:29,962 INFO] Epoch: 977 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:30,257 INFO] Epoch: 977 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:30,887 INFO] Epoch: 978 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:31,195 INFO] Epoch: 978 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:31,815 INFO] Epoch: 979 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 18:18:32,121 INFO] Epoch: 979 | ValAcc: 0.8980 | ValLoss: 0.0542 \n",
      "\n",
      "[2017-12-02 18:18:32,733 INFO] Epoch: 980 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:33,045 INFO] Epoch: 980 | ValAcc: 0.8980 | ValLoss: 0.0543 \n",
      "\n",
      "[2017-12-02 18:18:33,666 INFO] Epoch: 981 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:33,973 INFO] Epoch: 981 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:34,608 INFO] Epoch: 982 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:34,917 INFO] Epoch: 982 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:35,530 INFO] Epoch: 983 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:35,838 INFO] Epoch: 983 | ValAcc: 0.8970 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:36,457 INFO] Epoch: 984 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:36,767 INFO] Epoch: 984 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:37,407 INFO] Epoch: 985 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:37,715 INFO] Epoch: 985 | ValAcc: 0.8980 | ValLoss: 0.0542 \n",
      "\n",
      "[2017-12-02 18:18:38,335 INFO] Epoch: 986 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:38,661 INFO] Epoch: 986 | ValAcc: 0.8980 | ValLoss: 0.0542 \n",
      "\n",
      "[2017-12-02 18:18:39,304 INFO] Epoch: 987 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:39,610 INFO] Epoch: 987 | ValAcc: 0.8980 | ValLoss: 0.0542 \n",
      "\n",
      "[2017-12-02 18:18:40,241 INFO] Epoch: 988 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:40,549 INFO] Epoch: 988 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:41,169 INFO] Epoch: 989 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:41,483 INFO] Epoch: 989 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:42,097 INFO] Epoch: 990 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:42,408 INFO] Epoch: 990 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:43,035 INFO] Epoch: 991 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:43,345 INFO] Epoch: 991 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:43,968 INFO] Epoch: 992 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:44,282 INFO] Epoch: 992 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:44,907 INFO] Epoch: 993 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:45,216 INFO] Epoch: 993 | ValAcc: 0.8980 | ValLoss: 0.0543 \n",
      "\n",
      "[2017-12-02 18:18:45,838 INFO] Epoch: 994 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:46,133 INFO] Epoch: 994 | ValAcc: 0.8980 | ValLoss: 0.0541 \n",
      "\n",
      "[2017-12-02 18:18:46,759 INFO] Epoch: 995 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:47,065 INFO] Epoch: 995 | ValAcc: 0.8970 | ValLoss: 0.0542 \n",
      "\n",
      "[2017-12-02 18:18:47,684 INFO] Epoch: 996 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:47,987 INFO] Epoch: 996 | ValAcc: 0.8980 | ValLoss: 0.0540 \n",
      "\n",
      "[2017-12-02 18:18:48,607 INFO] Epoch: 997 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:48,919 INFO] Epoch: 997 | ValAcc: 0.8980 | ValLoss: 0.0542 \n",
      "\n",
      "[2017-12-02 18:18:49,536 INFO] Epoch: 998 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:49,845 INFO] Epoch: 998 | ValAcc: 0.8980 | ValLoss: 0.0543 \n",
      "\n",
      "[2017-12-02 18:18:50,477 INFO] Epoch: 999 | TrainAcc: 1.0000 | TrainLoss: 0.0000\n",
      "[2017-12-02 18:18:50,779 INFO] Epoch: 999 | ValAcc: 0.8980 | ValLoss: 0.0543 \n",
      "\n",
      "[2017-12-02 18:18:50,794 INFO] Train done.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwYAAAIqCAYAAACe310tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOkKJvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0ffe2898>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 277,
       "width": 387
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxMAAAIqCAYAAABfWBeBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a15ea0780>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 277,
       "width": 393
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Net()\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "def save_checkpoint(state, filename='./save/checkpoint_{}.pth'.format(ex_name)):\n",
    "    torch.save(state, filename)\n",
    "    #shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "    \n",
    "criterion = nn.KLDivLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "tracker = TrainingTracker()\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    tracker.epoch = epoch\n",
    "    \n",
    "    _train(model, train_loader, criterion, optimizer, tracker)\n",
    "    _validate(model, val_loader, criterion, tracker)\n",
    "    \n",
    "    # if latest accuracy is the best in history\n",
    "    if np.argmax(tracker.history['val_acc']) == len(tracker.history['val_acc']) - 1:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        })\n",
    "    \n",
    "logger.info('Train done.')\n",
    "tracker.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Pseudo Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:16<00:00, 16.43it/s]\n",
      "[2017-12-02 17:49:32,844 INFO] TestAcc: 0.9045 | TestLoss: 0.0309 \n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "resume = torch.load('./save/checkpoint_{}.pth'.format(ex_name))\n",
    "model.load_state_dict(resume['state_dict'])\n",
    "_test(model, test_loader, criterion, make_pl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:13<00:00, 20.20it/s]\n",
      "[2017-12-02 18:19:04,253 INFO] TestAcc: 0.8952 | TestLoss: 0.0376 \n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "resume = torch.load('./save/checkpoint_{}.pth'.format(ex_name))\n",
    "model.load_state_dict(resume['state_dict'])\n",
    "_test(model, test_loader, criterion, make_pl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
